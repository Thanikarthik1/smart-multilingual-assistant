{
  "0": " \nARTIFICIAL INTELLIGENCE \n[R20A0513] \nLECTURE NOTES \nB.TECH III YEAR – I SEM \n(R20) (2023-2024) \n \n \nMALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY \n(Autonomous Institution – UGC, Govt. of India) \nRecognized under 2(f) and 12 (B) of UGC ACT 1956 \n(Affiliated to JNTUH, Hyderabad, Approved by AICTE - Accredited by NBA & NAAC – ‘A’ Grade - ISO 9001:2015 Certified) \n \n \n \nMaisammaguda, Dhulapally (Post Via. Hakimpet), Secunderabad – 500100, Telangana State, India \n \n \nMALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY \nDepartment of Information Technology \n \n          III Year B.Tech. CSE-I Sem \nLT/P/D C \n \n \n3-/-/- 3 \n \n(R20A0513) ARTIFICIAL INTELLIGENCE \n \nCourse Objectives: \n \n1. To train the students to understand different types of AI agents. \n2. To understand various AI search algorithms. \n3. Fundamentals of knowledge representation, building of simple knowledge-basedsystems and to apply \nknowledge representation. \n4. Fundamentals of reasoning \n5. Study of Markov Models enable the student ready to step into applied AI. \n \n \n             UNIT - I \nIntroduction: AI problems, Agents and Environments, Structure of Agents, Problem Solving Agents \nBasic Search Strategies: Problem Spaces, Uninformed Search (Breadth First, Depth- First \nSearch,Depth-first with Iterative Deepening), Heuristic Search (Hill Climbing, Generic Best-First, A*), \nConstraint Satisfaction (Backtracking, Local Search) \n             UNIT - II \nAdvanced Search: Constructing Search Trees, Stochastic Search, AO* Search Implementation, Minimax \nSearch, Alpha-Beta Pruning. \nBasic Knowledge Representation and Reasoning: Propositional Logic, First-Order Logic, Forward \nChaining and Backward Chaining, Introduction to Probabilistic Reasoning, Bayes Theorem \n           UNIT - III \nAdvanced Knowledge Representation and Reasoning: Knowledge Representation Issues, Non- \nmonotonic Reasoning, Other Knowledge Representation Schemes. \nReasoning Under Uncertainty: Basic probability, Acting Under Uncertainty, Bayes‘ Rule, \nRepresenting Knowledge in an Uncertain Domain, Bayesian Networks \n \n            UNIT - IV \nLearning: What Is Learning? Rote Learning, Learning by Taking Advice, Learning in Problem Solving, \n           Learning from Examples - Winston‘s Learning Program, Decision Trees. \n \n           UNIT - V \nExpert Systems: Representing and Using Domain Knowledge, Shell, Explanation, \nKnowledge Acquisition. \n           TEXT BOOK: \n1. Russell, S. and Norvig, P, Artificial Intelligence: A Modern Approach, Third Edition, Prentice- \nHall, 2010 \n            REFERENCE BOOKS: \n1. \nArtificial Intelligence, Elaine Rich, Kevin Knight, Shivasankar B. Nair, The McGrawHill \npublications, Third Edition, 2009. \n2. \nGeorge F. Luger, Artificial Intelligence: Structures and Strategies for Complex Problem \nSolving, Pearson Education, 6th ed., 2009. \n             COURSE OUTCOMES: \n \n1. Understand the informed and uninformed problem types and apply search strategies tosolve them. \n2. Apply difficult real- l i fe problems in a state space representation so  as to solve thoseusing AI techniques \nlike searching and game playing. \n3. Design and evaluate intelligent expert models for perception and prediction fromintelligent environment. \n4. Formulate valid solutions for problems involving uncertain inputs or outcomes by usingdecision making \ntechniques. \n5. Demonstrate and enrich knowledge to select and apply AI tools to synthesizeinformation and develop \nmodels within constraints of application area \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nINDEX \nS.NO \nTitle \nPage No \n1 \nUNIT-I: Introduction to AI \n1 \n2 \nUninformed Search Strategies \n11 \n3 \nHeuristic Search \n18 \n4 \nConstraint Satisfaction \n29 \n5 \nUNIT-II: Mini Max \n40 \n6 \nAlpha–Beta Pruning \n42 \n7 \nAO* Search \n51 \n8 \nSyntax and Semantics of First-Order Logic \n59 \n9 \nForward Chaining and Backward Chaining \n63 \n10 \nProbabilistic Reasoning \n69 \n11 \nBayes Theorem \n72 \n12 \nUNIT-III: Knowledge Representation Issues \n79 \n13 \nActing Under Uncertainty \n83 \n14 \nBayes‘ Rule \n89 \n15 \nBayesian Networks \n90 \n16 \nUNIT-IV: Forms of Learning \n92 \n17 \nWinston‘s Learning Program \n96 \n18 \nDecision Trees \n98 \n19 \nUNIT-V: Representing and Using Domain Knowledge \n101 \n20 \nShell \n104 \n21 \nKnowledge Acquisition \n106 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 1 \n \nUNIT- I \n \n \nIntroduction: \n \n Artificial Intelligence is concerned with the design of intelligence in an artificial device. \nThe term was coined by John McCarthy in 1956. \n Intelligence is the ability to acquire, understand and apply the knowledge to achieve \ngoals in the world. \n AI is the study of the mental faculties through the use of computational models \n AI is the study of intellectual/mental processes as computational processes. \n AI program will demonstrate a high level of intelligence to a degree that equals \nor exceeds the intelligence required of a human in performing some task. \n AI is unique, sharing borders with Mathematics,  Computer Science, \nPhilosophy, Psychology, Biology, Cognitive Science and many others. \n Although there is no clear definition of AI or even Intelligence, it can be described as \nan attempt to build machines that like humans can think and act, able to learn and use \nknowledge to solve problems on their own. \nSub Areas of AI: \n \n1) Game Playing \nDeep Blue Chess program beat world champion Gary Kasparov \n2) Speech Recognition \nPEGASUS spoken language interface to American Airlines' EAASY SABRE reservation \nsystem, which allows users to obtain flight information and make reservations over the \n \ntelephone. The 1990s has seen significant advances in speech recognition so that limited \nsystems are now successful. \n3) Computer Vision \nFace recognition programs in use by banks, government, etc. The ALVINN system \nfrom CMU autonomously drove a van from Washington, D.C. to San Diego (all but 52 of \nIntroduction: AI problems, Agents and Environments, Structure of Agents, Problem Solving \nAgents Basic Search Strategies: Problem Spaces, Uninformed Search (Breadth First, Depth- \nFirst Search, Depth-first with Iterative Deepening), Heuristic Search (Hill Climbing, Generic \nBest-First, A*), Constraint Satisfaction (Backtracking, Local Search) \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 2 \n \n2,849 miles), averaging 63 mph day and night, and in all weather conditions. \nHandwriting \nrecognition, \nelectronics \nand \nmanufacturing \ninspection, \nphoto \ninterpretation, baggage inspection, reverse engineering to automatically construct a 3D \ngeometric model. \n4) Expert Systems \nApplication-specific systems that rely on obtaining the knowledge of human experts in an \narea and programming that knowledge into a system. \na. Diagnostic Systems: MYCIN system for diagnosing bacterial infections of the blood \nand suggesting treatments. Intellipath pathology diagnosis system (AMA approved). \nPathfinder medical diagnosis system, which suggests tests and makes diagnoses. \nWhirlpool customer assistance center. \nb. System Configuration \nDEC's XCON system for custom hardware configuration. Radiotherapy treatment planning. \nc. Financial Decision Making \nCredit card companies, mortgage companies, banks, and the U.S. government employ \nAI systems to detect fraud and expedite financial transactions. For example, \nAMEX credit check. \nd. Classification Systems \nPut information into one of a fixed set of categories using several sources of information. \nE.g., financial decision making systems. NASA developed a system for classifying very \nfaint areas in astronomical images into either stars or galaxies with very high accuracy by \nlearning from human experts' classifications. \n5) Mathematical Theorem Proving \nUse inference methods to prove new theorems. \n6) Natural Language Understanding \nAltaVista's translation of web pages. Translation of Catepillar Truck manuals into 20 languages \n7) Scheduling and Planning \nAutomatic scheduling for manufacturing. DARPA's DART system used in Desert Storm \nand Desert Shield operations to plan logistics of people and supplies. American Airlines \nrerouting contingency planner. European space agency planning and scheduling of \nspacecraft assembly, integration and verification. \n8) Artificial Neural Networks: \n9) Machine Learning \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 3 \n \nApplications of AI: \n \nAI algorithms have attracted close attention of researchers and have also been applied \nsuccessfully to solve problems in engineering. Nevertheless, for large and complex problems, \nAI algorithms consume considerable computation time due to stochastic feature of the search \napproaches \n \n1. \nBusiness; financial strategies \n2. \nEngineering: check design, offer suggestions to create new product, expert systems \nfor all engineering problems \n3. \nManufacturing: assembly, inspection and maintenance \n4. \nMedicine: monitoring, diagnosing \n5. \nEducation: in teaching \n6. \nFraud detection \n7. \nObject identification \n8. \nInformation retrieval \n9. \nSpace shuttle scheduling \n \nBuilding AI Systems: \n \n1)Perception \nIntelligent biological systems are physically embodied in the world and experience the \nworld through their sensors (senses). For an autonomous vehicle, input might be images \nfrom a camera and range information from a rangefinder. For a medical diagnosis system, \nperception is the set of symptoms and test results that have been obtained and input to the \nsystem manually. \n2)Reasoning \nInference, decision-making, classification from what is sensed and what the internal \"model\" \nis of the world. Might be a neural network, logical deduction system, Hidden Markov Model \ninduction, heuristic searching a problem space, Bayes Network inference, genetic algorithms, \netc. \nIncludes areas of knowledge representation, problem solving, decision theory, planning, \ngame theory, machine learning, uncertainty reasoning, etc. \n           3) Action \nBiological systems interact within their environment by actuation, speech, etc. All behavior \niscentered around actions in the world. Examples include controlling the steering of a Mars rover \nor autonomous vehicle, or suggesting tests and making diagnoses for a medical diagnosis \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 4 \n \nsystem. Includes areas of robot actuation, natural language generation, and speech synthesis. \nThe definitions of AI: \n \na) \"The exciting new effort to make \ncomputers think . . . machines with \nminds,in the full and literal sense\" \n(Haugeland, 1985) \n \n\"The automation of] activities that we \nassociate with human thinking, activities \nsuch \nas \ndecision-making, \nproblem \nsolving, learning...\"(Bellman, 1978) \nb) \"The study of mental faculties \nthrough the use of computational \nmodels\" \n(Charniak \nand \nMcDermott, 1985) \n \n\"The study of the computations \nthat make it possible to perceive, \nreason, and act\" (Winston, 1992) \nc) \"The art of creating machines that \nperform \nfunctions \nthat \nrequire \nintelligence \nwhen \nperformed \nby \npeople\" (Kurzweil, 1990) \n \n\"The study of how to make \ncomputers do things at which, at the \nmoment, people are better\" (Rich \nand Knight, 1 \n99 1 ) \nd) \"A field of study that seeks to \nexplain and emulate intelligent \nbehavior \nin \nterms \nof \ncomputational \nprocesses\" \n(Schalkoff, 1 990) \n\"The branch of computer science \nthat \nis \nconcerned \nwith \nthe \nautomation \nof \nintelligent \nbehavior\" \n(Luger and Stubblefield, 1993) \n \n \nThe definitions on the top, (a) and (b) are concerned with reasoning, whereas those on \nthe bottom, (c) and (d) address behavior. The definitions on the left, (a) and (c) measure \nsuccess in terms of human performance, and those on the right, (b) and (d) measure the \nideal concept of intelligence called rationality \nIntelligent Systems: \nIn order to design intelligent systems, it is important to categorize them into four \ncategories (Luger and Stubberfield 1993), (Russell and Norvig, 2003) \n1. \nSystems that think like humans \n2. \nSystems that think rationally \n3. \nSystems that behave like humans \n4. \nSystems that behave rationally \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 5 \n \n \nHuman- Like \nRationall y \n \n \nThin\nk: \n \nCognitive Science Approach \n \n“Machines that think like humans” \n \nLaws of thought Approach \n \n“ Machines that think Rationally” \n \n \nAct: \n \nTuring Test Approach \n \n“Machines \nthat \nbehave \nlike \nhumans” \n \nRational Agent Approach \n \n“Machines \nthat \nbehave\nRationally” \n \nCognitive Science: Think Human-Like \n \na. \nRequires a model for human cognition. Precise enough models allow \nsimulation by computers. \n \nb. \nFocus is not just on behavior and I/O, but looks like reasoning process. \n \nc. \nGoal is not just to produce human-like behavior but to produce a sequence of steps of \nthe reasoning process, similar to the steps followed by a human in solving the same task. \n \nLaws of thought: Think Rationally \n \na. The study of mental faculties through the use of computational models; that it is, \nthe study of computations that make it possible to perceive reason and act. \n \n  Focus is on inference mechanisms that are probably correct and guarantee an optimal solution. \n \nb. Goal is to formalize the reasoning process as a system of logical rules and procedures \nof inference. \n \nc. Develop systems of representation to allow inferences to be like \n \n―Socrates is a man. All men are mortal. Therefore Socrates is mortal” \nTuring Test: Act Human-Like \n \na. \nThe art of creating machines that perform functions requiring intelligence when \nperformed by people; that it is the study of, how to make computers do things which, at the \nmoment, people do better. \nb. \nFocus is on action, and not intelligent behavior centered around the representation of the world \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 6 \n \nc. \nExample: Turing Test \n              3 rooms contain: a person, a computer and an interrogator. \n \no \nThe interrogator can communicate with the other 2 by teletype (to avoid the machine \nimitate the appearance of voice of the person) \n \no \nThe interrogator tries to determine which the person is and which the machine is. \n \no \nThe machine tries to fool the interrogator to believe that it is the human, and the \nperson also tries to convince the interrogator that it is the human. \n \no \nIf the machine succeeds in fooling the interrogator, then conclude that the machine is \nintelligent. \n \nRational agent: Act Rationally \n \na. \nTries to explain and emulate intelligent behavior in terms of computational process; \nthat it is concerned with the automation of the intelligence. \nb. \nFocus is on systems that act sufficiently if not optimally in all situations. \n \nc. \nGoal is to develop systems that are rational and sufficient  \n \nAgents and Environments: \n \n \n \nFig 2.1: Agents and Environments \nAgent: \nAn Agent is anything that can be viewed as perceiving its environment through sensors \nand acting upon that environment through actuators. \n A human agent has eyes, ears, and other organs for sensors and hands, legs, mouth, \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 7 \n \nand other body parts for actuators. \n A robotic agent might have cameras and infrared range finders for sensors and \nvarious motors foractuators. \n A software agent receives keystrokes, file contents, and network packets as sensory \ninputs and acts on the environment by displaying on the screen, writing files, and \nsending network packets. \n \nPercept: \nWe use the term percept to refer to the agent's perceptual inputs at any given instant. \n \nPercept Sequence: \nAn agent's percept sequence is the complete history of everything the agent has ever perceived. \n \nAgent function: \nMathematically speaking, we say that an agent's behavior is described by the agent \nfunction that maps any given percept sequence to an action. \nAgent program \nInternally, the agent function for an artificial agent will be implemented by an \nagentprogram. It is important to keep these two ideas distinct. The agent function is an \nabstract \nmathematical description; the agent program is a concrete implementation, running on \nthe agent architecture. \nTo illustrate these ideas, we will use a very simple example-the vacuum-cleaner world shown \nin Fig 2.1.5. This particular world has just two locations: squares A and B. The vacuum \nagent perceives which square it is in and whether there is dirt in the square. It can choose to \nmove left, move right, suck up the dirt, or do nothing. One very simple agent function is the \nfollowing: if the current square is dirty, then suck, otherwise move to the other square. A \npartial tabulation of this agent function is shown in Fig 2.1.6. \n \n \nFig 2.1.5: A vacuum-cleaner world with just two locations. \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 8 \n \nAgent function \n \nPercept Sequence \nAction \n[A, Clean] \nRight \n[A, Dirty] \nSuck \n[B, Clean] \nLeft \n[B, Dirty] \nSuck \n[A, Clean], [A, Clean] \nRight \n[A, Clean], [A, Dirty] \nSuck \n… \n \nFig 2.1.6: Partial tabulation of a simple agent function for the example: vacuum-cleaner world shown \nin the Fig 2.1.5 \n \nFig 2.1.6(i): The REFLEX-VACCUM-AGENT program is invoked for each new percept \n(location, status) and returns an action each time \n \n \nA Rational agent is one that does the right thing. we say that the right action is the one \nthat will cause the agent to be most successful. That leaves us with the problem of deciding \nhow and when to evaluate the agent's success. \nWe use the term performance measure for the how—the criteria that determine how \nsuccessful an agent is. \n Ex-Agent cleaning the dirty floor \n Performance Measure-Amount of dirt collected \n When to measure-Weekly for better results \nFunction REFLEX-VACCUM-AGENT ([location, status]) returns an \naction If status=Dirty then return Suck \nelse if location = A then return Right \nelse if location = B then return Left \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 9 \n \nWhat is rational at any given time depends on four things: \n \nThe performance measure defining the criterion of success \n \nThe agent‘s prior knowledge of the environment \n \nThe actions that the agent can perform \n \nThe agent‘s percept sequence up to now. \n \nOmniscience ,Learning and Autonomy: \n We need to distinguish between rationality and omniscience. An Omniscient agent \nknows the actual outcome of its actions and can act accordingly but omniscience is \nimpossible in reality. \n Rational agent not only gathers information but also learns as much as possible \nfrom what it perceives. \n If an agent just relies on the prior knowledge of its designer rather than its own \npercepts then the agent lacks autonomy. \n A system is autonomous to the extent that its behavior is determined its own experience. \n A rational agent should be autonomous. \n \n \nE.g., a clock(lacks autonomy) \n No input (percepts) \n Run only but its own algorithm (prior knowledge) \n No learning, no experience, etc. \n \nENVIRONMENTS: \nThe Performance measure, the environment and the agents actuators and sensors comes under \nthe \nheading \ntask \nenvironment. \nWe \nalso \ncall \nthis \nas \nPEAS(Performance,Environment,Actuators,Sensors) \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 10 \n \n \n \nEnvironment-Types: \n \n1. Accessible vs. inaccessible or Fully observable vs Partially Observable: \nIf an agent sensor can sense or access the complete state of an environment at each point \nof time then it is a fully observable environment, else it is partially observable. \n2. Deterministic vs. Stochastic: \nIf the next state of the environment is completely determined by the current state and the \nactions selected by the agents, then we say the environment is deterministic \n3. Episodic vs. nonepisodic: \n The agent's experience is divided into \"episodes.\" Each episode consists of the agent \nperceiving and then acting. The quality of its action depends just on the episode itself, \nbecausesubsequent episodes do not depend on what actions occur in previous episodes. \n Episodic environments are much simpler because the agent does not need to think ahead. \n4. Static vs. dynamic. \nIf the environment can change while an agent is deliberating, then we say the \nenvironment is dynamic for that agent; otherwise it is static. \n5. Discrete vs. continuous: \nIf there are a limited number of distinct, clearly defined percepts and actions we say that \nthe environment is discrete. Otherwise, it is continuous. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 11 \n \n \n \nSTRUCTURE OF INTELLIGENT AGENTS \n \n The job of AI is to design the agent program: a function that implements the agent \nmapping from percepts to actions. We assume this program will run on some sort of \nARCHITECTURE computing device, which we will call the architecture. \n The architecture might be a plain computer, or it might include special-purpose \nhardware for certain tasks, such as processing camera images or filtering audio input. It \nmight also include software that provides a degree of insulation between the raw computer \nand the agent program, so that we can program at a higher level. In general, the \narchitecture makes the percepts from the sensors available to the program, runs the \nprogram, and feeds the program's action choices to the effectors as they are generated. \n The relationship among agents, architectures, and programs can be summed up as \nfollows: agent = architecture + program \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 12 \n \n     Agent programs: \n Intelligent agents accept percepts from an environment and generates actions. The \nearly versions of agent programs will have a very simple form (Figure 2.4) \n Each will use some internal data structures that will be updated as new percepts arrive. \n These data structures are operated on by the agent's decision-making procedures to \ngenerate an action choice, which is then passed to the architecture to be executed \n \nTypes of agents: \nAgents can be grouped into four classes based on their degree of perceived intelligence and capability : \n Simple Reflex Agents \n Model-Based Reflex Agents \n Goal-Based Agents \n Utility-Based Agents \nSimple reflex agents: \n Simple reflex agents ignore the rest of the percept history and act only on the \nbasis of the current percept. \n The agent function is based on the condition-action rule. \n If the condition is true, then the action is taken, else not. This agent function only succeeds \nwhen the environment is fully observable. \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 13 \n \nModel-based reflex agents: \n \n                  The Model-based agent can work in a partially observable environment, and track the \nsituation. \n A model-based agent has two important factors: \n Model: It is knowledge about \"how things happen in the world,\" so it is called a Model-based agent. \n Internal State: It is a representation of the current state based on percept history. \n \nGoal-based agents: \n A goal-based agent has an agenda. \n It operates based on a goal in front of it and makes decisions based on how best to reach that goal. \n A goal-based agent operates as a search and planning function, meaning it targets the goal \nahead and finds the right action in order to reach it. \n Expansion of model-based agent. \n \n \nUtility-based agents: \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 14 \n \n  utility-based agent is an agent that acts based not only on what the goal is, but the best way to \nreach that goal. \n The Utility-based agent is useful when there are multiple possible alternatives, and an \nagent has to choose in order to perform the best action. \n The term utility can be used to describe how \"happy\" the agent is. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nProblem Solving Agents: \n Problem solving agent is a goal-based agent. \n Problem solving agents decide what to do by finding sequence of actions that lead to desirable states. \nGoal Formulation: \nIt organizes the steps required to formulate/ prepare one goal out of multiple goals available. \nProblem Formulation: \nIt is a process of deciding what actions and states to consider to follow goal \nformulation. The process of looking for a best sequence to achieve a goal is called \nSearch. \nA search algorithm takes a problem as input and returns a solution in the form of action \nsequences. Once the solution is found the action it recommends can be carried out. This is \ncalled Execution phase. Well Defined problems and solutions: \nA problem can be defined formally by 4 components: \n The initial state of the agent is the state where the agent starts in. In this case, the initial state can be \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 15 \n \ndescribed as In: Arad \n The possible actions available to the agent, corresponding to each of the state the agent \nresides in. \nFor example, ACTIONS(In: Arad) = {Go: Sibiu, Go: Timisoara, Go: \nZerind}. Actions are also known as operations. \n A description of what each action does.the formal name for this is Transition model,Specified \nby the function Result(s,a) that returns the state that results from the action a in state s. \nWe also use the term Successor to refer to any state reachable from a given state by a single \naction. For EX:Result(In(Arad),GO(Zerind))=In(Zerind) \n \n \nTogether the initial state,actions and transition model implicitly defines the state space of the \nproblem State space: set of all states reachable from the initial state by any sequence of \nactions \n The goal test, determining whether the current state is a goal state. Here, the goal state is \n{In: Bucharest} \n The path cost function, which determine the cost of each path, which is reflecting \nin the performance measure. \nwe define the cost function as c(s, a, s‘), where s is the current state and a is the action \nperformed by the agent to reach state s‘. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 16 \n \nExample – \n8 puzzle problem \nInitial State \nGoal State \n \n States: a state description specifies the location of each of the eight tiles in one of the \nnine squares. For efficiency, it is useful to include the location of the blank. \n Actions: blank moves left, right, up, or down. \n Transition Model: Given a state and action, this returns the resulting state. For example \nif we apply left to the start state the resulting state has the 5 and the blank switched. \n Goal test: state matches the goal configuration shown in fig. \n Path cost: each step costs 1, so the path cost is just the length of the path. \nState Space Search/Problem Space Search: \nThe state space representation forms the basis of most of the AI methods. \n \nFormulate a problem as a state space search by showing the legal problem states, the legal \noperators, and the initial and goal states. \n \nA state is defined by the specification of the values of all attributes of interest in the world \n \nAn operator changes one state into the other; it has a precondition which is the value \nof certain attributes prior to the application of the operator, and a set of effects, which arethe \nattributes altered by the operator \n \nThe initial state is where you start \n \nThe goal state is the partial description of the solution \n \nFormal Description of the problem: \n1. Define a state space that contains all the possible configurations of the relevant objects. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 17 \n \n2. Specify one or more states within that space that describe possible situations from \nwhich the problem solving process may start ( initial state) \n3. Specify one or more states that would be acceptable as solutions to the problem. ( goal states) \nSpecify a set of rules that describe the actions (operations) available \n \nState-Space Problem Formulation: \n \nExample: A problem is defined by four items: \n1. \ninitial state e.g., \"at Arad― \n2. \nactions or successor function : \nS(x) = set of action–\nstate pairs e.g., S(Arad) = {<Arad → Zerind, Zerind>, … } \n3. \ngoal test (or set of goal states) \ne.g., x = \"at Bucharest‖, Checkmate(x) \n4. \npath cost (additive) \ne.g., sum of distances, number of actions executed, etc. \nc(x,a,y) is the step cost, assumed to be ≥ 0 \nA solution is a sequence of actions leading from the initial state to a goal state \n \n \n \n \nExample: 8-queens problem \n \nInitial State: Any arrangement of 0 to 8 queens on board. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 18 \n \n1. \nOperators: add a queen to any square. \n2. \nGoal Test: 8 queens on board, none attacked. \n3. \nPath cost: not applicable or Zero (because only the final state counts, search cost might \nbe of interest). \nSearch strategies: \nSearch: Searching is a step by step procedure to solve a search-problem in a given search \nspace. A search problem can have three main factors: \nSearch Space: Search space represents a set of possible solutions, which a system may \nhave. Start State: It is a state from where agent begins the search. \nGoal test: It is a function which observe the current state and returns whether the goal state is \nachieved or not. \n                   Properties of Search Algorithms \n \nWhich search algorithm one should use will generally depend on the \nproblem domain. There are four important factors to consider: \n1. \nCompleteness – Is a solution guaranteed to be found if at least one solution exists? \n \n2. \nOptimality – Is the solution found guaranteed to be the best (or lowest cost) solution \nif there exists more than one solution? \n3. \nTime Complexity – The upper bound on the time required to find a solution, as a \nfunction of the complexity of the problem. \n4. \nSpace Complexity – The upper bound on the storage space (memory) required at any \npoint during the search, as a function of the complexity of the problem. \n \nState Spaces versus Search Trees: \n \nState Space \no \nSet of valid states for a problem \no \nLinked by operators \no \ne.g., 20 valid states (cities) in the Romanian travel problem \n \nSearch Tree \n– \nRoot node = initial state \n– \nChild nodes = states that can be visited from parent \n– \nNote that the depth of the tree can be infinite \n• \nE.g., via repeated states \n– \nPartial search tree \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 19 \n \n• \nPortion of tree that has been expanded so far \n– \nFringe \n• \nLeaves of partial search tree, candidates for expansion \nSearch trees = data structure to search state-space \n      Searching \nMany traditional search algorithms are used in AI applications. For complex problems, the \ntraditional algorithms are unable to find the solution within some practical time and space \nlimits. Consequently, many special techniques are developed; using heuristic functions. The \nalgorithms that use heuristic functions are called heuristic algorithms. Heuristic algorithms \nare not really intelligent; they appear to be intelligent because they achieve better \nperformance. \n \n         Heuristic algorithms are more efficient because they take advantage of feedback from the data t o direct \nthe search path. \nUninformed search \nAlso called blind, exhaustive or brute-force search, uses no information about the problem to \nguide the search and therefore may not be very efficient. \nInformed Search: \n \nAlso called heuristic or intelligent search, uses information about the problem to guide the \nsearch, usually guesses the distance to a goal state and therefore efficient, but the search may not \nbe always possible. \n     Uninformed Search (Blind searches \n Breadth First Search: \n One simple search strategy is a breadth-first search. In this strategy, the root node is \nexpanded first, then all the nodes generated by the root node are expanded next, and then their \nsuccessors, and so on. \n In general, all the nodes at depth d in the search tree are expanded before the nodes at depth d \n+ FS illustrated: \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 20 \n \nStep 1: Initially frontier contains only one node corresponding to the source state A. \n \nFigure 1 \nFrontier: A \n \nStep 2: A is removed from fringe. The node is expanded, and its children B and C are \ngenerated. They are placed at the back of fringe. \nFigure 2 \nFrontier: \nB \nC \n \nStep 3: Node B is removed from fringe and is expanded. Its children D, E are generated \nand put at the back of fringe. \nFigure 3 \nFrontier: C D E \n \nStep 4: Node C is removed from fringe and is expanded. Its children D and G are added \nto the back of fringe. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 21 \n \n \n \nFigure 4 \nFrontier: D E D G \n \nStep 5: Node D is removed from fringe. Its children C and F are generated and added to the \nback of fringe. \nFigure 5 \nFrontier: E D G C F \n \nStep 6: Node E is removed from fringe. It has no children. \n \nFigure 6 \nFrontier: D G C F \n \nStep 7: D is expanded; B and F are put in OPEN. \nFigure 7 \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 22 \n \nFrontier: G C F B F \nStep 8: G is selected for expansion. It is found to be a goal node. So the algorithm returns \nthe path A C G by following the parent pointers of the node corresponding to G. The \nalgorithm terminates. \nBreadth first search is: \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAdvantages\nOne of the simplest search strategies \n \nComplete. If there is a solution, BFS is guaranteed to find it. \n \nIf there are multiple solutions, then a minimal solution will be found \n \nThe algorithm is optimal (i.e., admissible) if all operators have the \nsame cost. Otherwise, breadth first search finds a solution with the shortest \npath length. \n \nTime complexity \n: O(bd ) \n \nSpace complexity \n: O(bd ) \n \nOptimality \n:Yes \nb - branching factor(maximum no of successors of \nany node), d – Depth of the shallowest goal node \nMaximum length of any path (m) in search space \n \n BFS will provide a solution if any solution exists. \n If there are more than one solutions for a given problem, then BFS will provide the minimal \nsolution which requires the least number of steps. \n \nDisadvantages: \n Requires the generation and storage of a tree whose size is exponential the depth of \nthe shallowest goal node. \n The breadth first search algorithm cannot be effectively used unless the search space \nis quite small. \nApplications Of Breadth-First Search Algorithm \nGPS Navigation systems: Breadth-First Search is one of the best algorithms used to find \nneighboring locations by using the GPS system. \nBroadcasting: Networking makes use of what we call as packets for communication. These \npackets follow a traversal method to reach various networking nodes. One of the most \ncommonly used traversal \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 23 \n \nmethods is Breadth-First Search. It is being used as an algorithm that is used to communicate \nbroadcasted packets across all the nodes in a network. \nDepth- First- Search. \nWe may sometimes search the goal along the largest depth of the tree, and move up only \nwhen further traversal along the depth is not possible. We then attempt to find alternative \noffspring of the parent of the node (state) last visited. If we visit the nodes of a tree using \nthe above principles to search the goal, the traversal made is called depth first traversal and \nconsequently the search strategy is called depth first search. \n \n \nDFS illustrated: \n \nA State Space Graph Step 1: Initially fringe contains \nonly the node for A. \n \nFigure 1 \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 24 \n \nFRINGE: A \n \nStep 2: A is removed from fringe. A is expanded and its children B and C are put in front \nof fringe. \n \nFigure 2 \nFRINGE: B C \n \nStep 3: Node B is removed from fringe, and its children D and E are pushed in front of fringe. \nFigure 3 \nFRINGE: D E  \nStep 4: Node D is removed from fringe. C and F are pushed in front of fringe. \nFigure 4 \nFRINGE: C F E C \n \nStep 5: Node C is removed from fringe. Its child G is pushed in front of fringe. \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 25 \n \nFigure 5 \n \n \n \n \nFigure 5 \n \nFRINGE: G F E C \nStep 6: Node G is expanded and found to be a goal node. \n \n \nFigure 6 \nFRINGE: G F E C \n \nThe solution path A-B-D-C-G is returned and the algorithm terminates. \n \nDepth first search \n1. \ntakes exponential time. \n2. \nIf N is the maximum depth of a node in the search space, in the worst case the algorithm will \nd \ntake time O(b ). \n3. \nThe space taken is linear in the depth of the search tree, O(bN). \n \nNote that the time taken by the algorithm is related to the maximum depth of the search tree. If \nthe search tree has infinite depth, the algorithm may not terminate. This can happen if the \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 26 \n \nsearch space is infinite. It can also happen if the search space contains cycles. The latter case \ncan be handled by checking for cycles in the algorithm. Thus Depth First Search is not \ncomplete. \n \n \nIterative Deeping DFS \n The iterative deepening algorithm is a combination of DFS and BFS algorithms. \n This search algorithm finds out the best depth limit and does it by gradually increasing \nthe limit until a goal is found. \n This algorithm performs depth-first search up to a certain \"depth limit\", and it keeps \nincreasing the depth limit after each iteration until the goal node is found. \n \nAdvantages: \n It combines the benefits of BFS and DFS search algorithm in terms of fast search and \nmemory efficiency. \nDisadvantages: \n The main drawback of IDDFS is that it repeats all the work of the previous phase. \n \nIterative deepening search L=0 \n \n            Iterative \ndeepening search L=1 \n \nIterative deepening search L=2 \n \n                                     \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 27 \n \nIterativeDeepeningSea\nrchL=3 \n \n \nM is the goal node. So we stop there. \nCompl\nete: \nYes \nTime: \nO(bd) \nSpace: \nO(bd) \nOptimal: Yes, if step cost = 1 or increasing function of depth. \nConclusion: \nWe can conclude that IDS is a hybrid search strategy between BFS and DFS inheriting \ntheir advantages. \nIDS is faster than BFS and DFS. \nItissaidthat ―IDSisthepreferreduniformedsearchmethodwhen thereisalargesearchspace and the depth of \nthe solution is not know \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 28 \n \n \n          Informed search/Heuristic search \n \nA heuristic is a method that \n \n might not always find the best solution but is guaranteed to find a good solution in \nreasonable time. By sacrificing completeness it increases efficiency. \n Useful in solving tough problems which \no \ncould not be solved any other way. \no \nsolutions take an infinite time or very long time to compute. \nCalculating Heuristic Value: \n \n \n1. Euclidian distance- used to calculate straight line distance. \n \n2.Manhatten distance-If we want to calculate vertical or horizontal distance For ex: \n8 puzzle problem \nSource state \n \n1 \n3 \n2 \n6 \n5 \n4 \n \n8 \n7 \ndestination state \n \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n \n \nThen the Manhattan distance would be sum of the no of moves required to move each \nnumber from source state to destination state. \n \nNumber \nin \n8 \npuzzle \n2 \n3 \n4 \n5 \n6 \n7 \n8 \nNo. \nof \nmoves to \nreach \ndestinati\non \n2 \n1 \n2 \n0 \n2 \n2 \n0 \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 29 \n \n3. No. of misplaced tiles for 8 puzzle problem \n \nSource state \n \n1 \n3 \n2 \n6 \n5 \n4 \n \n8 \n7 \n \nDestination state \n \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n \nHere just calculate the number of tiles that have to be changed to reach \ngoal state Here 1,5,8 need not be changed \n2,3,4,6,7 should be changed, so the heuristic value will be 5(because 5 tiles have to be changed) \n \n \nHill Climbing Algorithm \n \n Hill climbing algorithm is a local search algorithm which continuously moves in the \ndirection of increasing elevation/value to find the peak of the mountain or best solution to the \nproblem. It terminates when it reaches a peak value where no neighbor has a higher value. \n It is also called greedy local search as it only looks to its good immediate neighbor state \nand not beyond that. \n Hill Climbing is mostly used when a good heuristic is available. \n In this algorithm, we don't need to maintain and handle the search tree or graph as it only \nkeeps a single current state. \nThe idea behind hill climbing is as follows. \n \n1. \nPick a random point in the search space. \n2. \nConsider all the neighbors of the current state. \n3. \nChoose the neighbor with the best quality and move to that state. \n4. \nRepeat 2 thru 4 until all the neighboring states are of lower quality. \n5. \nReturn the current state as the solution state. \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 30 \n \n \n \n \nDifferent regions in the state space landscape: \n \nLocal Maximum: Local maximum is a state which is better than its neighbor states, but there is \nalso another state which is higher than it. \n \nGlobal Maximum: Global maximum is the best possible state of state space landscape. It has the \nhighest value of objective function. \n \nCurrent state: It is a state in a landscape diagram where an agent is currently present. \n \nFlat local maximum: It is a flat space in the landscape where all the neighbor states of current states have the \nsame value. \n \nShoulder: It is a plateau region which has an uphill edge. \n \nAlgorithm for Hill Climbing \n \nProblems in Hill Climbing Algorithm: \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 31 \n \n \n \n \n \n \nSimulated annealing search \nA hill-climbing algorithm that  never makes ―downhill‖  moves towards states with lower \nvalue  (or higher cost) is guaranteed to be incomplete, because it can stuck on a local \nmaximum. In contrast, a purely random walk –that is, moving to a successor chosen uniformly \nat random from the set of successors – is complete, but extremely inefficient. Simulated \nannealing is an \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 32 \n \nalgorithm that combines hill-climbing with a random walk in some way that yields both \nefficiency and completeness. \nsimulated annealing algorithm is quite similar to hill climbing. Instead of picking the best \nmove, however, it picks the random move. If the move improves the situation, it is always \naccepted. Otherwise, the algorithm accepts the move with some probability less than 1. \nThe probability decreases exponentially with the ―badness‖ of the move – the amount E \nby which the evaluation is worsened. The probability also decreases as the \"temperature\" T \ngoes down: \"bad moves are more likely to be allowed at the start when temperature is high, \nand they become more unlikelyas T decreases. One can prove that if the schedule lowers T \nslowly enough, the algorithm will find a global optimum with probability approaching 1. \nSimulated annealing was first used extensively to solve VLSI layout problems. It has been \napplied widely to factory scheduling and other large-scale optimization tasks. \n \nBest First Search: \n \n \nA combination of depth first and breadth first searches. \n \nDepth first is good because a solution can be found without computing all nodes \nand breadth first is good because it does not get trapped in dead ends. \n \nThe best first search allows us to switch between paths thus gaining the benefit of both \napproaches. At each step the most promising node is chosen. If one of the nodes chosen \ngenerates nodes that are less promising it is possible to choose another at the same level \nand in effect the search changes from depth to breadth. If on analysis these are no better \nthan this previously unexpanded node and branch is not forgotten and the search method \nreverts to the \n \nOPEN is a priority queue of nodes that have been evaluated by the heuristic function but \nwhich have not yet been expanded into successors. The most promising nodes are at the \nfront. \n \nCLOSED are nodes that have already been generated and these nodes must be stored \nbecause a graph is being used in preference to a tree. \nAlgorithm: \n1. Start with OPEN holding the initial state \n \n                                   Until a goal is found or there are no nodes left on open do. \n \n \nPick the best node on OPEN \n \nGenerate its successors \n \nFor each successor Do \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 33 \n \n• \nIf it has not been generated before ,evaluate it ,add it to OPEN and record its \nparent \n \n• \nIf it has been generated before change the parent if this new path is better and in that \ncase update the cost of getting to any successor nodes. \n \n2. If a goal is found or no more nodes left in OPEN, quit, else return to 2. \n \nExample: \n \n1. \nIt is not optimal. \n2. \nIt is incomplete because it can start down an infinite path and never return to try \nother possibilities. \n3. \nThe worst-case time complexity for greedy search is O (bm), where m is the maximum \ndepth of the search space. \n4. \nBecause greedy search retains all nodes in memory, its space complexity is the same \nas itstime complexity \nA* Algorithm \n \nThe Best First algorithm is a simplified form of the A* algorithm. \n \nThe A* search algorithm (pronounced \"Ay-star\") is a tree search algorithm that finds a path \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 34 \n \nfrom a given initial node to a given goal node (or one passing a given goal test). It employs a \n\"heuristic estimate\" which ranks each node by an estimate of the best route that goes through \nthatnode. It visits the nodes in order of this heuristic estimate. \n \nSimilar to greedy best-first search but is more accurate because A* takes into account the \nnodes that have already been traversed. \n \nFrom A* we note that f = g + h where \n \ng is a measure of the distance/cost to go from the initial node to the current node \n \nhis an estimate of the distance/cost to solution from the current node. \n \nThus fis an estimate of how long it takes to go from the initial node to the solution \n \nAlgorithm: \n \n1. Initialize \n: \nSet \nOPEN = (S); CLOSED \n= ( ) g(s)= 0, f(s)=h(s) \n2. Fail \n: \nIf \nOPEN = ( ), Terminate and fail. \n \n3. Select \n: \n select the minimum cost state, n, \nfrom OPEN, save n in CLOSED \n4. Terminate  : \nIf n €G, Terminate with success and return f(n) \n \n5. Expand \n: \nfor each successor, m, of n \n \na) If m € [OPEN U CLOSED] Set g(m) = g(n) + c(n , m) \nSet f(m) = g(m) + h(m) \nInsert m in OPEN \n \nb) If m € [OPEN U CLOSED] \n \nSet g(m) = min { g(m) , g(n) + c(n \n, m)} Set f(m) = g(m) + h(m) \nIf f(m) has decreased and m € CLOSED \n \nMove m to OPEN. \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 35 \n \nDescription: \n \nA* begins at a selected node. Applied to this node is the \"cost\" of entering this node \n(usually zero for the initial node). A* then estimates the distance to the goal node fromthe \ncurrent node. This estimate and the cost added together are the heuristic which is assigned \nto the path leading to this node. The node is then added to a priority queue, oftencalled \n\"open\". \n \nThe algorithm then removes the next node from the priority queue (because of the way a \npriority queue works, the node removed will have the lowest heuristic). If the queue is empty, \nthere is no path from the initial node to the goal node and the algorithm stops. If the node is the \ngoal node, A* constructs and outputs the successful path and stops. \n \nIf the node is not the goal node, new nodes are created for all admissible adjoining nodes; \nthe exact way of doing this depends on the problem at hand. For each successive node, A* \ncalculates the \"cost\" of entering the node and saves it with the node. This cost is calculated \nfrom the cumulative sum of costs stored with its ancestors, plus the cost of the operation which \nreached this new node. \n \nThe algorithm also maintains a 'closed' list of nodes whose adjoining nodes have been \nchecked. If a newly generated node is already in this list with an equal or lower cost, no \nfurther processing is done on that node or with the path associated with it. If a node in the \nclosed list matches the new one, but has been stored with a higher cost, it is removed from the \nclosed list, and processing continues on the new node. \n \n \nNext, an estimate of the new node's distance to the goal is added to the cost to form the heuristic for that         \nnode. This is then added to the 'open' priority queue, unless an identical node is found there. \n \nOnce the above three steps have been repeated for each new adjoining node, the original \nnode taken from the priority queue is added to the 'closed' list. The next node is then popped \nfrom the priority queue and the process is repeatedThe heuristic costs from each city to \nBucharest: \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 36 \n \n \n \n \n \n \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 37 \n \n \nA* search properties: \n \n \nThe algorithm A* is admissible. This means that provided a solution exists, the first \nsolution found by A* is an optimal solution. A* is admissible under the following \nconditions: \n \nHeuristic function: for every node n , h(n) ≤ h*(n) . \n \n \nA* is also complete. \n \n \nA* is optimally efficient for a given heuristic. \n \n \nA* is much more efficient that uninformed search. \n \nConstraint Satisfaction Problems \nhttps://www.cnblogs.com/RDaneelOlivaw/p/8072603.html \n \nSometimes a problem is not embedded in a long set of action sequences but requires picking the \nbest option from available choices. A good general-purpose problem solving technique is to list \nthe constraints of a situation (either negative constraints, like limitations, or positive elements that \nyou want in the final solution). Then pick the choice that satisfies most of the constraints. \nFormally speaking, a constraint satisfaction problem (or CSP) is defined by a set of \nvariables, X1;X2; : : : \n;Xn, and a set of constraints, C1;C2; : : : ;Cm. Each variable Xi has anonempty domain Di of \npossible values. Each constraint Ci involves some subset of tvariables and specifies the \nallowable combinations of values for that subset. A state of theproblem is defined by an \nassignment of values to some or all of the variables, {Xi = vi;Xj =vj ; : : :} An assignment that \ndoes not violate any constraints is called a consistent or \nlegalassignment. A complete assignment is one in which every variable is mentioned, and \nasolution to a CSP is a complete assignment that satisfies all the constraints. Some CSPs also \nrequire a solution that maximizes an objectivefunction. \nCSP can be given an incremental formulation as a standard search problem as follows: \n \n1. \nInitial state: the empty assignment fg, in which all variables are unassigned. \n \n2. \nSuccessor function: a value can be assigned to any unassigned variable, provided that \nit does not conflict with previously assigned variables. \n3. \nGoal test: the current assignment is complete. \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 38 \n \n4. \nPath cost: a constant cost for every step \n \nExamples: \n \n1. \nThe best-known category of continuous-domain CSPs is that of linear programming \nproblems, where constraints must be linear inequalities forming a convex region. \n \n2. \nCrypt arithmetic puzzles. \n \nExample: The map coloring problem. \n \nThe task of coloring each region red, green or blue in such a way that no \nneighboring regions have the same color. \nWe are given the task of coloring each region red, green, or blue in such a way \nthat the neighboring regions must not have the same color. \nTo formulate this as CSP, we define the variable to be the regions: WA, NT, Q, NSW, V, SA, and \nT. The domain of each variable  is  the  set  {red,  green,  blue}.  The constraints require \n \nneighboring regions to have distinct colors: for example, the allowable combinations \nfor WA \nand \nNT \nare \nthe \npairs \n{(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,green)}. \n(The \nconstraint can also berepresented as the inequality WA ≠NT). Therearemany possible \nsolutions, \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 39 \n \nsuch as {WA = red, NT = green, Q = red, NSW = green, V = red, SA = blue, T = \nred}.Map of Australia showing each of its states and territories \nConstraint Graph: A CSP is usually represented as an undirected graph, called \nconstraint graph where the nodes are the variables and the edges are the \nbinaryconstraints. \n \n \nThe map-coloring problem represented as a \nconstraint graph. CSP can be viewed as a standard \nsearch problem as follows: \n> \nInitial state : the empty assignment {},in which all variables are unassigned. \n> \nSuccessor function: a value can be assigned to any unassigned variable, \nprovided that it does not conflict with previously assigned variables. \n> \nGoal test: the current assignment is complete. \n> \nPath cost: a constant cost(E.g.,1) for every step. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 40 \n \nUNIT II \n \n \nConstructing Search Trees: \n \n \n \nGame Playing \nAdversarial search, or game-tree search, is a technique for analyzing an adversarial game in \norder to try to determine who can win the game and what moves the players should make in \norder to win. Adversarial search is one of the oldest topics in Artificial Intelligence. The \noriginal ideas for adversarial search were developed by Shannon in 1950 and independently by \nTuring in 1951, in the context of the game of chess— and their ideas still form the basis for the \ntechniques used today. \n2- \nPerson Games: \no \nPlayers: We call them Max and Min. \no \nInitial State: Includes board position and whose turn it is. \n \n \nAdvanced Search: Constructing Search Trees, Stochastic Search, AO* Search Implementation, Minimax \nSearch, Alpha-Beta Pruning Basic Knowledge Representation and Reasoning: Propositional Logic, First- \nOrder Logic, Forward Chaining and Backward Chaining, Introduction to Probabilistic Reasoning, Bayes \nTheorem \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 41 \n \no \nOperators: These correspond to legal moves. \no \nTerminal Test: A test applied to a board position which determines whether the game is \nover. In chess, for example, this would be a checkmate or stalemate situation. \no \nUtility Function: A function which assigns a numeric value to a terminalstate. For example, \nin chess the outcome is win (+1), lose (-1) or draw (0). Note that by convention,we always \nmeasure utility relative to Max. \n \nMini Max Algorithm: \n1. \nGenerate the whole game tree. \n2. \nApply the utility function to leaf nodes to get their values. \n3. \nUse the utility of nodes at level n to derive the utility of nodes at level n-1. \n4. \nContinue backing up values towards the root (one layer at a time). \n5. \nEventually the backed up values reach the top of the tree, at which point Max chooses the \nmove that yields the highest value. This is called the minimax decision because it maximises \nthe utility for Max on the assumption that Min will play perfectly to minimise it. \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 42 \n \nExample: \n \n \nExample: \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 43 \n \n \n \n \n \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 44 \n \n \n \n \n \nProperties of minimax: \n \n \nComplete \n: \nYes (if tree is finite) \n \nOptimal : \nYes (against an optimal opponent) \n \nTime complexity \n: \nO(bm) \n \nSpace complexity \n: \nO(bm) (depth-first exploration) \n \nFor chess, b ≈ 35, m ≈100 for \"reasonable\" games \n→ exact solution  completely  infeasible. \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 45 \n \nLimitations \n– \nNot always feasible to traverse entire tree \n– \nTime limitations \n \nAlpha-Beta pruning algorithm: \n \n• \nPruning: eliminating a branch of the search tree from consideration without exhaustive \nexamination of each node \n• \n - Pruning: the basic idea is to prune portions of the search tree that cannot \nimprovethe utility value of the max or min node, by just considering the values of nodes \nseen sofar. \n• \nAlpha-beta pruning is used on top of minimax search to detect paths that do not need \nto be explored. The intuition is: \n• \nThe MAX player is always trying to maximize the score. Call this \n. \n• \nThe MIN player is always trying to minimize the score. Call this \n. \n• \nAlpha cutoff: Given a Max node n, cutoff the search below n (i.e., don't generate \nor examine any more of n's children) if alpha(n) >= beta(n) \n(alpha increases and passes beta from below) \n• \nBeta cutoff.: Given a Min node n, cutoff the search below n (i.e., don't generate \nor examine any more of n's children) if beta(n) <= alpha(n) \n(beta decreases and passes alpha from above) \n• \nCarry alpha and beta values down during search Pruning occurs whenever alpha >= beta \n \n \nAlgorithm: \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 46 \n \n \nExample: \n \n \n1) Setup phase: Assign to each left-most (or right-most) internal node of \nthe tree, variables: alpha = -infinity, beta = +infinity \n \n \n \n2) Look at first computed final configuration value. \nIt’s a 3. Parent is a min \nnode, so set the beta (min) value to 3. \n \n \n \n \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 47 \n \n3) Look   at next value, 5.    Since parent is a min   node,   we   want the   minimum of      \n3 and 5 which is 3. Parent min node is done – fill alpha (max) value of its parent max node. \nAlways set alpha for max nodes and beta for min nodes. Copy the state of the max parent node \ninto the second unevaluated min child. \n \n \n4) Look at next value, 2. Since parent node is min with b=+inf, 2 is smaller, change b. \n \n \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 48 \n \n5) Now, the min parent node has a max value of 3 and min value of 2. The value of the 2nd \nchild does not matter. If it is >2, 2 will be selected for min node. If it is <2, it will be \nselected for min node, but since it is <3 it will not get selected for the parent max node. Thus, \nwe prune the right subtree of the min node. Propagate max value up the tree. \n \n6) Max node is now done and we can set the beta value of its parent and \npropagate node state to sibling subtree’s left-most path. \n \n7) The next node is 10. 10 is not smaller than 3, so state of parent does not change. \nWe still have to look at the 2nd child since alpha is still –inf. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 49 \n \n \n \n8) The next node is 4. Smallest value goes to the parent min node. Min subtree is \ndone, so the parent max node gets the alpha (max) value from the child. Note that if \nthe max node had a 2nd subtree, we can prune it since a>b. \n \n \n \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 50 \n \n9) Continue propagating value up the tree, modifying the corresponding alpha/beta \nvalues. Also propagate the state of root node down the left-most path of the right \nsubtree. \n \n \n10) Next value is a 2. We set the beta (min) value of the min parent to 2. Since no \nother children exist, we propagate the value up the tree. \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 51 \n \n11) We have a value for the 3rd level max node, now we can modify the beta (min) \nvalue of the min parent to 2. Now, we have a situation that a>b and thus the value of \nthe rightmost subtree of the min node does not matter, so we prune the whole subtree. \n \n \n12) Finally, no more nodes remain, we propagate values up the tree. The root has  a \nvalue of 3 that comes from the left-most child. Thus, the player should choose the left-\nmost child’s move in order to maximize his/her winnings. As you can see, the result is \nthe same as with the mini-max example, but we did not visit all nodes of the tree. \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 52 \n \nAO* Search: (And-Or) Graph: \n \n \nAO* is informed search algorithm ,work based on heuristic. We already know about the \ndivide and conquer strategy, a solution to a problem can be obtained by decomposing it into \nsmaller sub-problems. \n \nEach of this sub-problem can then be solved to get its sub solution. These sub solutions \ncan then recombined to get a solution as a whole. That is called is Problem Reduction. AND-\nOR graphs or AND – OR trees are used for representing the solution. \n \nThis method generates arc which is called as AND-OR arcs. One AND arc may point to \nany number of successor nodes, all of which must be solved in order for an arc to point to a \nsolution. AND-OR graph is used to represent various kind of complex problem solutions. \n \nAO* search algo. is based on AND-OR graph so ,it is called AO* search algo. \n \nExample: In Following figure , we have taken example of Goal: Acquire TV Set. This goal or \nproblem is subdivided into two subproblems or sub goals like 1) STEAL TV SET 2) Earn some \nmoney, Buy TV SET. SO to solve this problem if we select second alternative of earn some \nMoney, then along with that Buy TV SET also need to select as it is part of and graph. \n \nWhereas First alternative :Steal Tv Set is forming OR Graph \n \n \n \nAO * Search Algorithm In Artificial Intelligence \n \n Just as in an OR graph, several arcs may emerge from a single node, indicating a variety of ways \nin which the original problem might be solved. \n This is why the structure is called not simply an OR-graph but rather an AND-OR graph (which also \nhappens to be an AND-OR tree) \n \nAO * Search Algorithm In Artificial Intelligence With Example \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 53 \n \n \nAO * Search Algorithm In Artificial Intelligence \n \n \nAn algorithm to find a solution in an AND – OR graph must handle AND area appropriately. \n \nA* algorithm can not search AND – OR graphs efficiently. \n \nThis can be understand from the give figure \n \nIn figure (a) the top node A has been expanded producing two area one leading to B and leading to \nC-D \n. the numbers at each node represent the value of f ‗ at that node (cost of getting to the goal state \nfrom current state). For simplicity, it is assumed that every operation(i.e. applying a rule) has unit \ncost, i.e., each are with single successor will have a cost of 1 and each of its components. \n \n \nWith the available information till now , it appears that C is the most promising node to \nexpand since its f ‗ = 3 , the lowest but going through B would be better since to use C we must \nalso use D‘ and the cost would be 9(3+4+1+1). Through B it would be 6(5+1). \n \nThus the choice of the next node to expand depends not only on a value but also on whether \nthat node is part of the current best path form the initial mode. Figure (b) makes this clearer. In \nfigure the node G appears to be the most promising node, with the least f ‗ value. But G is not on \nthe current beat path, since to use G we must use GH with a cost of 9 and again this demands that \narcs be used (with a cost of 27). \n \nThe path from A through B, E-F is better with a total cost of (17+1=18). Thus we can see \nthat to search an AND-OR graph, the following three things must be done. \n \n1. traverse the graph starting at the initial node and following the current best path, and \naccumulate the set of nodes that are on the path and have not yet been expanded. \n2. Pick one of these best unexpanded nodes and expand it. Add its successors to the graph and \ncompute f ‗ (cost of the remaining distance) for each of them. \n \n3. Change the f ‗ estimate of the newly expanded node to reflect the new information \nproduced by its successors. Propagate this change backward through the graph. Decide which \nof the current best path. \n \nThe propagation of revised cost estimation backward is in the tree is not necessary in A* \nalgorithm. This is because in AO* algorithm expanded nodes are re-examined so that the \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 54 \n \ncurrent best path can be selected. \n \nAdvantages of AO*: \n \n \nIt is Complete \n \nWill not go in infinite loop \n \nLess Memory Required \nDisadvantages of AO*: \nIt is not optimal as it does not explore all the path once it find a solution. \n \nBASIC KNOWLEDGE REPRESENTATION AND REASONING: \n \n• Humans are best at understanding, reasoning, and interpreting knowledge. Human \nknows things, which is knowledge and as per their knowledge they perform various \nactions in the real world. \n• But how machines do all these things comes under knowledge representation \n \n• \nThere are three factors which are put into the machine, which makes it valuable: \n• \nKnowledge: The information related to the environment is stored in the machine. \n• \nReasoning: The ability of the machine to understand the stored knowledge. \n• \nIntelligence: The ability of the machine to make decisions on the basis of the stored information. \n• \nA knowledge representation language is defined by two aspects: \n• \nThe syntax of a language describes the possible configurations that can constitute sentences. \n• \nThe semantics determines the facts in the world to which the sentences refer. \n• \nFor example, the syntax of the language of arithmetic expressions says that if x and y \nare expressions denoting numbers, then x > y is a sentence about numbers. The semantics of \nthe language says that x > y is false when y is a bigger number than x, and true otherwise \nFrom the syntax and semantics, we can derive an inference mechanism for an agent that \nuses the language. \n• \nRecall that the semantics of the language determine the fact to which a given sentence \nrefers. Facts are part of the world, \n• \n whereas their representations must be encoded in some way that can be physically \nstored within an agent. We cannot put the world inside a computer (nor can we put it inside a \nhuman), so all reasoning mechanisms must operate on representations of facts, rather than on \nthe facts themselves. Because sentences are physical configurations of parts of the agent, \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 55 \n \nReasoning must be a process of constructing new physical configurations from old ones. \nProper reasoning should ensure that the new configurations represent facts that actually \nfollow from the facts that the old configurations represent. \n• \nWe want to generate new sentences that are necessarily true, given that the old sentences are true \n \n \nThis relation between sentences is called entailment. \n• \nIn mathematical notation, the relation of entailment between a knowledge base KB and a \nsentence a is pronounced \"KB entails a\" and written as \n• \nAn inference procedure can do one of two things: \n• \ngiven a knowledge base KB, it can generate new sentences a that purport to be entailed by KB. \n• \nE.g., x + y = 4 entails 4 = x + y \n• \nEntailment is a relationship between sentences (i.e., syntax) that is based on semantics \n \nPROPOSITIONAL LOGIC: \n• \nPropositional logic (PL) is the simplest form of logic where all the statements are \nmade by propositions. \n• \nA proposition is a declarative statement which is either true or false. \nIt is a technique of knowledge representation in logical and mathematical form \n \nSyntax of propositional logic: \n• \nThe symbols of prepositional logic are the logical constants True and False, proposition \nsymbols such as P and Q, the logical connectives A, V, <=>, =>and and parentheses, \n• \nAll sentences are made by putting these symbols together using the following rules: \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 56 \n \n• \nThe logical constants True and False are sentences by themselves. \n• \nA prepositional symbol such as P or Q is a sentence by itself. \n• \nWrapping parentheses around a sentence yields a sentence, for example, (P A Q). \n \n                A sentence can be formed by combining simpler sentences with one of the five logical \nconnectives:: \n1. Negation: A sentence such as ¬ P is called negation of P. A literal can be either Positive \nliteral or negative literal. \nExample:P=Today is not Sunday -> ¬ p \n1. Conjunction: A sentence which has 𝖠 connective such as, P 𝖠 Q is called a \nconjunction. Example: Rohan is intelligent and hardworking. It can be written as, \nP= Rohan is intelligent, \nQ= Rohan is hardworking. → P𝖠 Q. \n2. Disjunction: A sentence which has ∨ connective, such as P ∨ Q. is called disjunction, where \nP and Q are the propositions. \n3. Example: \"Ritika is a doctor or Engineer\", \nHere P= Ritika is Doctor. Q= Ritika is Doctor, so we can write it as P ∨ Q. \n4. 4. Implication: A sentence such as P → Q, is called an implication. Implications are also \nknown as if-then rules. It can be represented as \nIf it is raining, then the street is wet. \nLet P= It is raining, and Q= Street is wet, so it is represented as P → Q \n5. 5. Biconditional: A sentence such as P⇔ Q is a Biconditional sentence, example \nIf I am breathing, then I am alive \nP= I am breathing, Q= I am alive, it can be represented as P ⇔ Q. \nPrecedence of connectives: \nPrecedence \nOperators \nFirst \nPrecedence\n \nParenthesis \nSecond \nPrecedence\n \nNegation \nThird \nPrecedence\n \nConjunction(AND) \nFourth \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 57 \n \nPrecedence\n \nDisjunction(OR) \nFifth \nPrecedence\n \nImplication \nSix Precedence \nBiconditiona \n \nPrecedence of connectives: \n \nSemantics \n• \nThe semantics of prepositional logic is also quite straightforward. We define it by \nspecifying the interpretation of the proposition symbols and constants, and specifying the \nmeanings of the logical connectives. \nValidity \n• \nTruth tables can be used not only to define the connectives, but also to test for valid sentences. \n• \nGiven a sentence, we make a truth table with one row for each of the possible combinations of \ntruth values for the proposition symbols in the sentence. \n• \nIf the sentence is true in every row, then the sentence is valid. For example, the sentence \n((P V H) A ¬H) => P \nTranslating English into logic: \n• \nUser defines semantics of each propositional symbol \n• \nP: It is Hot \n• \nQ: It is Humid \n• \nR:It is raining \n1. If it is humid then it is \nhot Q->P \n.If it is hot and humid , then it is \nraining (P A Q)->R \nLimitations of Propositional logic: \n• \nIn propositional logic, we can only represent the facts, which are either true or false. \n• \nPL is not sufficient to represent the complex sentences or natural language statements. \n• \nThe propositional logic has very limited expressive power. \n• \nConsider the following sentence, which we cannot represent using PL logic. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 58 \n \n• \n\"Some humans are intelligent\", or \"Sachin likes cricket \n \n             First-order logic: \n \nAdvantages of Propositional Logic \n \n The declarative nature of propositional logic, specify that knowledge and inference are \nseparate, and inference is entirely domain-independent.   Propositional logic is a declarative \nlanguage because its semantics is based on a truth relation between sentences and possible worlds. \n It also has sufficient expressive power to deal with partial information, using disjunction \nand negation. \n Propositional logic has a third COMPOSITIONALITY property that is desirable in \nrepresentation languages, namely, compositionality. In a compositional language, the meaning \nof a sentence is a function of the meaning of its parts. For example, the meaning of ―S1,4𝖠 \nS1,2‖ is related to the meanings of ―S1,4‖ and ―S1,2. \n \nDrawbacks of Propositional Logic \nPropositional logic lacks the expressive power to concisely describe an environment with many objects. \n \nFor example, we were forced to write a separate rule about breezes and pits for each square, \nsuch as B1,1⇔ (P1,2 ∨P2,1) . \nIn English, itseemseasyenoughtosay,―Squares adjacenttopitsarebreezy.‖ \nThe syntax and semantics of English somehow make it possible to describe the environment concisely \n \nSYNTAX AND SEMANTICS OF FIRST-ORDER LOGIC \n \nModels for first-order logic : \n \nThe models of a logical language are the formal structures that constitute the possible worlds \nunder consideration. Each model links the vocabulary of the logical sentences to elements of the \npossible world, so that the truth of any sentence can be determined. Thus, models for \npropositional logic link proposition symbols to predefined truth values. Models for first-order \nlogic have objects. The domain of a model is the set of objects or domain elements it contains. \nThe domain is required to be nonempty—every possible world must contain at least one object. \nA relation is just the set of tuples of objects that are related. \n \nUnary Relation: Relations relates to single Object Binary Relation: Relation Relates to multiple \nobjects Certain kinds of relationships are best considered as functions, in that a given object must \nbe related to exactly one object. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 59 \n \n \nFor Example: \n \nRichard the Lionheart, King of England from 1189 to 1199; His younger brother, the evil King \nJohn, who ruled from 1199 to 1215; the left legs of Richard and John; crown \n \nUnary Relation : John is a king Binary Relation :crown is on head of john , Richard is brother \nofjohn The unary \"left leg\" function includes the following mappings: (Richard the Lionheart) -\n>Richard's left leg (King John) ->Johns left Leg \n \nSymbols and interpretations \n \nSymbols are the basic syntactic elements of first-order logic. Symbols stand for objects, \nrelations, and functions. \nThe symbols are of three kinds: Constant symbols which stand for objects; Example: John, \nRichard Predicate symbols, which stand for relations; Example: OnHead, Person, King, and \nCrown \nFunction symbols, which stand for functions. Example: left leg Symbols will begin with uppercase \nletters. Interpretation The semantics must relate sentences to models in order to determine truth. For \nthis to happen, we need an interpretation that specifies exactly which objects, relations and \nfunctions are referredto by the constant, predicate, and function symbols. \n \n             For Example: \n \nRichard refers to Richard the Lionheart and John refers to the evil king John. Brother refers to the \nbrotherhood relation OnHead refers to the \"on head relation that holds between the crown and King \nJohn; Person, King, and Crown refer to the sets of objects that are persons, kings, and crowns. \nLeftLeg refers to the \"left leg\" function, \n \nThe truth of any sentence is determined by a model and an interpretation for the sentence's symbols. \nTherefore, entailment, validity, and so on are defined in terms of all possiblemodels and all possible \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 60 \n \ninterpretations. The number of domain elements in each model may be unbounded-for example, the \ndomain elements may be integers or real numbers. Hence, the number of possible models is \nanbounded,as is the number of interpretations. \n \nTerm \n \nA term is a logical expression that refers to an object. Constant symbols are therefore terms. \nComplex Terms A complex term is just a complicated kind of name. A complex term is \nformed by a function symbol followed by a parenthesized list of terms as arguments to the \nfunction symbol For example: \"King John's left leg\" Instead of using a constant symbol, we use \nLeftLeg(John). The formal semantics of terms Consider a term f (tl,. . . , t,). The function symbol \nfrefers to some function in the model (F); the argument terms refer to objects in the domain (call \nthem d1….dn); and the termas a wholerefers to the object that is the value of the function Fapplied \nto dl, . . . , d,. For example,: the LeftLeg function symbol refers to the function ― (King John) \n-+ John's left leg‖ and John refers to King John, then LeftLeg(John) refers to King John's left leg. In \nthis way, the interpretation fixes the referent of every term. \nAtomic sentences \n \nAn atomic sentence is formed from a predicate symbol followed by a parenthesized list of \nterms: For Example: Brother(Richard, John). \nAtomic sentences can have complex terms as arguments. For Example: Married \n(Father(Richard), Mother( John)). \nAn atomic sentence is true in a given model, under a given interpretation, if the relation \nreferred to by the predicate symbol holds among the objects referred to by the arguments \nComplex sentences Complex sentences can be constructed using logical Connectives, just as in \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 61 \n \npropositional calculus. For Example: \n \n \n \nThus, the sentence says, ―For all x, if x is a king, then x is a person.‖ The symbol x is called  \na variable. Variables are lowercase letters. A variable is a term all by itself, and can also \nserve as the argument of a function A term with no variables is called a ground term. \nAssume we can extend the interpretation in different ways: x→ Richard the Lionheart, x→ King John, \nx→ Richard‘s left leg, x→ John‘s left leg, x→ the crown \n \nThe universally quantified sentence ∀x King(x) ⇒Person(x) is true in the original model if the sentence \nKing(x) \n⇒Person(x) is true under each of the five extended interpretations. That is, the universally quantified \nsentence is equivalent to asserting the following five sentences: \nRichard the Lionheart is a king ⇒Richard the Lionheart is a person. King John is a king ⇒King John \nis a person. Richard‘sleftlegisaking⇒Richard‘sleftleg is aperson. John‘s left legisaking⇒John‘s left \nleg is a person. The crown is a king ⇒the crown is a person. \nExistential quantification (∃) \n \nUniversal quantification makes statements about every object. Similarly, we can make a statement \nabout some object in the universe without naming it, by using an existential quantifier. \n―The sentence ∃x P says that P is true for at least one object x. More precisely, ∃x P is true in a given \nmodel if P is true in at least oneextended interpretationthat assigns x to a domain element.‖ ∃x is \npronounced ―There exists an x such that . . \n.‖ or ―For some x . . .‖. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 62 \n \nFor example, that King John has a crown on his head, we write ∃xCrown(x) 𝖠OnHead(x, John) Given \nassertions: \n \nRichard the Lionheart is a crown 𝖠Richard the Lionheart is on John‘s head; King John is a crown \n𝖠King Johnison John‘s head; Richard‘s left legisacrown𝖠Richard‘s leftlegison John‘s head; John‘s left leg is a \ncrown 𝖠John‘s left legis on John‘s head; The crown is a crown 𝖠the crown is on John‘s head. The fifth \nassertion is true in the model, so the original existentially quantified sentence is true in the model. Just \nas ⇒appears to be the natural connective to use with ∀, 𝖠is the natural connective to use with ∃. \nNested quantifiers \n \nOne can express more complex sentences using multiple quantifiers. \n \nFor example, ―Brothers are siblings‖ can be written as ∀x∀y Brother (x, y) ⇒Sibling(x, y). \nConsecutive quantifiers of the same type can be written as one quantifier with several variables. \nFor example, to say that siblinghood is a symmetric relationship, we can write∀x, y Sibling(x, y) \n⇔Sibling(y, x). In other cases we will have mixtures. \nFor example: 1. ―Everybody loves somebody‖ means that for every person, there is someone that person \nloves: ∀x∃y Loves(x, y) . 2. On the other hand, to say ―There is someone who is loved by everyone,‖ we \nwrite∃y∀x Loves(x, y) . \nConnections between ∀and ∃ \n \nUniversal and Existential quantifiers are actually intimately connected with each other, through negation. \n \n \nExample assertions: \n1. ― Everyone dislikes medicine‖ is the same as asserting ― there does not exist someone who likes medicine‖ , and  vice  \nversa:―∀x \n￢Likes(x, medicine)‖ is equivalent to ―￢∃x Likes(x,medicine)‖. \n2. ―Everyonelikesicecream‖ \nmeansthat― \nthereisnoonewhodoesnotlikeice \ncream‖ \n:∀xLikes(x,  \nIceCream)  is equivalent to ￢∃x ￢Likes(x, IceCream) . \nBecause ∀is really a conjunction over the universe of objects and ∃is a disjunction \nthat they obey De Morgan‘s rules. The De Morgan rules for quantified and \nunquantified sentences are as follows: \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 63 \n \n \n \nEquality \n \nFirst-order logic includes one more way to make atomic sentences, other than using a predicateand \nterms .We can use the equality symbol to signify that two terms refer to the same object. \nFor example, \n \n―Father(John) =Henry‖ says that the object referred to by Father (John) and the \nobject referred to by Henry are the same. \nBecause an interpretation fixes the referent of any term, determining the truth of an equality sentence is \nsimply a matter of seeing that the referents of the two terms are the same object.The equality symbol \ncan be used to state facts about a given function.It can also be used with negation to insist that two \nterms are not the same object. \nFor example, \n \n―Richard has at least two brothers‖ can be written as, ∃x, y Brother (x,Richard ) 𝖠Brother (y,Richard \n) \n𝖠￢\n(x=\ny) . \nThe \nsent\nence \n∃x, y Brother (x,Richard ) 𝖠Brother (y,Richard ) does not have the intended meaning. \nIn particular, it is true only in the model where Richard has only one brother considering the extended \ninterpretation in which both x and y are assigned to King John. The addition of ￢(x=y) rules out such \nmodels. \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 64 \n \n \n \n \nUSING FIRST ORDER LOGIC Assertions and queries in first-order \nlogic Assertions: \nSentences are added to a knowledge base using TELL, exactly as in propositional logic. Such \nsentences are called assertions. \nFor example, \n \nJohn is a king, TELL (KB, King (John)). Richard is a person. TELL (KB, Person (Richard)). \nAll kings are persons: TELL (KB, ∀x King(x) ⇒Person(x)). \nAsking Queries: \n \nWe can ask questions of the knowledge base using ASK. Questions asked with ASK are \ncalled queries or goals. \nFor example, \n \nASK (KB, King (John)) returns true. \n \nAny query that is logically entailed by the knowledge base should be answered \naffirmatively. Fo rexample, given the two preceding assertions, the query: \n―ASK (KB, Person (John))‖ should also return true. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 65 \n \nSubstitution or binding list \n \nWe can ask quantified queries, such as ASK (KB, ∃x Person(x)) . \n \nThe answer is true, but this is perhaps not as helpful as we would like. It is rather like answering \n―Can you tell me the time?‖ with ―Yes.‖ \n \nIf we want to know what value of x makes the sentence true, we will need a different \nfunction, ASKVARS, which we call with ASKVARS (KB, Person(x)) and which yields a \nstream of answers. \n \nIn this case there will be two answers: {x/John} and {x/Richard}. Such an answer is called a \nsubstitution or binding list. \n \nASKVARS is usually reserved for knowledge bases consisting solely of Horn clauses, because in \nsuch knowledge bases every way of making the query true will bind the variables to specific values. \n \nThe kinship domain \n \nThe objects in Kinship domain are people. \n \nWe have two unary predicates, Male and Female. \n \nKinship relations—parenthood, brotherhood, marriage, and so on—are represented by \nbinarypredicates: Parent, Sibling, Brother,Sister,Child, Daughter, Son, Spouse, Wife, Husband, \nGrandparent,Grandchild, Cousin, Aunt, and Uncle. \n \nWe use functions for Mother and Father, because every person has exactly one of each of these. \n \nWe can represent each function and predicate, writing down what we know in termsof the other symbols. \n \n \nFor example:- \n1. one‘s mother is one‘s female parent: ∀m, c Mother (c)=m ⇔Female(m) 𝖠Parent(m, \n. \n2. One‘s husband is one‘s male spouse: ∀w, h Husband(h,w) ⇔Male(h) 𝖠Spouse(h,w) . \n \n3. Male and female are disjoint categories: ∀xMale(x) ⇔￢Female(x) . \n \n4. Parent and child are inverse relations: ∀p, c Parent(p, c) ⇔Child (c, p) . \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 66 \n \n \n5. A grandparent is a parent of one‘s parent: ∀g, c Grandparent (g, c) ⇔∃p Parent(g, p) 𝖠Parent(p, c) \n \n \n A sibling is another child of one‘s parents: ∀x, y Sibling(x, y) ⇔x _= y 𝖠∃p Parent(p, x) \n𝖠Parent(p, \n \nAxioms: \n \nEach of these sentences can be viewed as an axiom of the kinship domain. Axioms are \ncommonly associated with purely mathematical domains. They provide the basic factual \ninformation from which useful conclusions can be derived. \n \nKinship axioms are also definitions; they have the form ∀x, y P(x, y) ⇔. . .. \n \nThe axioms define the Mother function, Husband, Male, Parent, Grandparent, and Sibling \npredicates in terms of other predicates. \n \nOur definitions ―bottom out‖ at a basic set of predicates (Child, Spouse, and Female) in terms \nof which the others are ultimately defined. This is a natural way in which to build up the \nrepresentation of a domain, and it is analogous to the way in which software packages are built \nup by successive definitions of subroutines from primitive library functions. \nTheorems: \n \nNot all logical sentences about a domain are axioms. Some are theorems—that is, they are \nentailed by the axioms. \n \nFor example, consider the assertion that siblinghood is symmetric: ∀x, y Sibling(x, y) ⇔Sibling(y, x) . \n \nIt is a theorem that follows logically from the axiom that  defines siblinghood. If we ASK the \nknowledge base this sentence, it should return true. From a purely logical point of view, a \nknowledge base need contain only axioms and no theorems, because the theorems do not increase \nthe set of conclusions that follow from the knowledge base. From a practical point of view, \ntheorems are essential to reduce the computational cost of deriving new sentences. Without them, \na reasoning system has to start from first principles every time. \n \nAxioms :Axioms without Definition \n \nNot all axioms are definitions. Some provide more general information about certain predicates \nwithout \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 67 \n \nconstituting a definition. Indeed, some predicates have no complete definition because we do not \nknow enough to characterize them fully. \n \nFor example, there is no obvious definitive way to complete the sentence \n \n∀xPerson(x) ⇔. . . \n \nFortunately, first-order logic allows us to make use of the Person predicate without completely \ndefining it. Instead, we can write partial specifications of properties that every person has and \nproperties that make something a person: \n \n∀xPerson(x) ⇒. . . ∀x . . . ⇒Person(x) . \n \nAxioms can also be ―just plain facts,‖ such as Male (Jim) and Spouse (Jim, Laura).Such facts \nform the descriptions of specific problem instances, enabling specific questions to be answered. \nThe answers to these questions will then be theorems that follow from the axioms \n \nNumbers, sets, and lists Number theory \n \nNumbers are perhaps the most vivid example of how a large theory can be built up from \nNATURAL NUMBERS a tiny kernel of axioms. We describe here the theory of natural numbers or \nnon-negative integers. We need: \n \npredicate \nNatNum that will be true of natural numbers; \nPEANO AXIOMS constant symbol, 0; One function symbol, S (successor). The Peano \naxioms define natural numbers and addition. \n \nNatural numbers are defined recursively: NatNum(0) . ∀n NatNum(n) ⇒ NatNum(S(n)) . \n \nThat is, 0 is a natural number, and for every object n, if n is a natural number, then S(n) is a natural \nnumber. \n \nSo the natural numbers are 0, S(0), S(S(0)), and so on. We also need axioms to constrain \nthe successor function: ∀n 0 != S(n) . ∀m, n m != n ⇒ S(m) != S(n) . \n \nNow we can define addition in terms of the successor function: ∀m NatNum(m) ⇒ + (0, m) = m . \n∀m, n NatNum(m) 𝖠 NatNum(n) ⇒ + (S(m), n) = S(+(m, n)) \n \nThe first of these axioms says that adding 0 to any natural number m gives m itself. \nAddition is represented using the binary function symbol ―+‖ in the term + (m, 0); \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 68 \n \nTo make our sentences about numbers easier to read, we allow the use of infix notation. We \ncan also write S(n) as n + 1, so the second axiom becomes : \n \n∀m, n NatNum (m) 𝖠 NatNum(n) ⇒ (m + 1) + n = (m + n)+1 . \n \nThis axiom reduces addition to repeated application of the successor function. Once we have \naddition, it is straightforward to define multiplication as repeated addition, exponentiation as \nrepeated multiplication, integer division and remainders, prime numbers, and so on. Thus, the \nwhole of number theory (including cryptography) can be built up from one constant, one \nfunction, one predicate and four axioms. \n \nSets \n \nThe domain of sets is also fundamental to mathematics as well as to commonsense reasoning. Sets \ncan be represented as individualsets, including empty sets. \n \nSets can be built up by: \nadding an element to a set or \nTaking the union or intersection of \ntwo sets. Operations that can be \nperformed on sets are: \nTo know whether an element is a member of a set Distinguish sets from objects that are not sets. \n \nVocabulary of set theory: \n \nThe empty set is a constant written as { }. There is one unary predicate, Set, which is true of \nsets. The binary predicates are \n \nx∈ s (x is \na member of set s) s1 ⊆ s2 ( set s1 is a subset, not necessarily proper, of set s2). \n \nThe binary functions are \n \ns1 ∩ s2 (the intersection of two sets), s1 𝖴 s2 (the union of two sets), and {x|s} (the set \nresulting from adjoining element x to set s). \n \nForward Chaining and backward chaining in AI \n \nInference engine: \n \nThe inference engine is the component of the intelligent system in artificial intelligence, which \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 69 \n \napplies logical rules to the knowledge base to infer new information from known facts. The \nfirst inferenceengine was part of the expert system. Inference engine commonly proceeds in two \nmodes, which are: \n \na. \nForward chaining \nb. Backward chaining \nHorn \nClause \nand \nDefinite clause: \nHorn clause and definite clause are the forms of sentences, which enables knowledge base to use a \nmore restricted and efficient inference algorithm. Logical inference algorithms use forward \nand backward chaining approaches, which require KB in the form of the first-order definite \nclause. \n \nDefinite clause: A clause which is a disjunction of literals with exactly one positive literal is \nknown as a definite clause or strict horn clause. \n \nHorn clause: A clause which is a disjunction of literals with at most one positive literal is \nknown as horn clause. Hence all the definite clauses are horn clauses. \n \nExample: (¬ p V ¬ q V k). It has only one positive \nliteral k. It is equivalent to p 𝖠 q → k. \nA. Forward Chaining \n \nForward chaining is also known as a forward deduction or forward reasoning method when \nusing an inference engine. Forward chaining is a form of reasoning which start with atomic \nsentences in the knowledge base and applies inference rules (Modus Ponens) in the forward \ndirection to extract more data until a goal is reached. \n \nThe Forward-chaining algorithm starts from known facts, triggers all rules whose premises \nare satisfied, and add their conclusion to the known facts. This process repeats until the \nproblem is solved. \n \nProperties of Forward-Chaining: \n \no \nIt is a down-up approach, as it moves from bottom to top. \no \nIt is a process of making a conclusion based on known facts or data, by starting from the \ninitial state and reaches the goal state. \no \nForward-chaining approach is also called as data-driven as we reach to the goal using available \n \n \ndata. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 70 \n \no \nForward -chaining approach is commonly used in the expert system, such as CLIPS, \nbusiness, and production rule systems. \nConsider the following famous example which we will use in both approaches: \n \nFacts Conversion into FOL: \no \nIt is a crime for an American to sell weapons to hostile nations. (Let's say p, q, and r are variables) \nAmerican (p) 𝖠 weapon(q) 𝖠 sells (p, q, r) 𝖠 hostile(r) → Criminal(p) ...(1) \no \nCountry A has some missiles. ?p Owns(A, p) 𝖠 Missile(p). It can be written in two definite \nclauses by \nusing \nExistential \nInstantiation, \nintroducing \nnew\n \nConstant \nT1. Owns(A, \nT1) (2) \nMissile(T1) ...................... (3) \no \nAll of \nthe \nmissiles \nwere \nsold \nto \ncountry \nA \nby \nRobert. \n?p Missiles(p) 𝖠 Owns (A, p) → Sells (Robert, p, A) ............ (4) \no \nMissiles \nare \nweapons. \nMissile(p) → Weapons (p)..................... (5) \no \nEnemy \nof \nAmerica \nis \nknown \nas \nhostile. \nEnemy(p, America) →Hostile(p)......................(6) \no \nCountryA \nis \nan \nenemy \nof \nAmerica. \nEnemy (A, America) ....................... (7) \no \nRobertisAmerican \nAmerican(Robert) ........................ (8) Forward chaining proof: \nStep-1: \n \nIn the first step we will start with the known facts and will choose the sentences which do \nnot have implications, such as: American(Robert), Enemy(A, America), Owns(A, T1), \nand Missile(T1). All these facts will be represented as below. \n \n \nStep-2: \n \nAt the second step, we will see those facts which infer from available facts and with \nsatisfied premises. Rule-(1) does not satisfy premises, so it will not be added in the first \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 71 \n \niteration. \nRule-(2) and (3) are already added. \n \nRule-(4) satisfy with the substitution {p/T1}, so Sells (Robert, T1, A) is added, which infers from \nthe conjunction of Rule (2) and (3). \n \nRule-(6) is satisfied with the substitution(p/A), so Hostile(A) is added and which infers from Rule-(7). \n \n \nStep-3: \n \nAt step-3, as we can check Rule-(1) is satisfied with the substitution {p/Robert, q/T1, r/A}, \nso we can add Criminal(Robert) which infers all the available facts. And hence we reached our \ngoal statement. \n \nHence it is proved that Robert is Criminal using forward chaining approach. \n \nBackward Chaining: \nBackward-chaining is also known as a backward deduction or backward reasoning method when \nusing an inference engine. A backward chaining algorithm is a form of reasoning, which starts with \nthe goal and works backward, chaining through rules to find known facts that support the goal. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 72 \n \n \nProperties of backward chaining: \n \no \nIt is known as a top-down approach. \no \nBackward-chaining is based on modus ponens inference rule. \no \nIn backward chaining, the goal is broken into sub-goal or sub-goals to prove the facts true. \no \nIt is called a goal-driven approach, as a list of goals decides which rules are selected and used. \no \nBackward -chaining algorithm is used in game theory, automated theorem proving \ntools, inference engines, proof assistants, and various AI applications. \no \nThe backward-chaining method mostly used a depth-first search strategy for proof. \n \nExample: \n \nIn backward-chaining, we will use the same above example, and will rewrite all the rules. \n \no \nAmerican   (p)   𝖠   weapon(q)   𝖠 \nsells   (p,   q, r)   𝖠 hostile(r) \n \n \n→\n \nCriminal(p) ...(1) Owns(A, T1) \n(2) \no \nMissile(T1) \no \n?p Missiles(p) 𝖠 Owns (A, p) → Sells (Robert, p, A) ..................... (4) \no \nMissile(p) → Weapons (p) ..............................(5) \no \nEnemy(p, America) →Hostile(p) .............................. (6) \no \nEnemy (A, America) ............................... (7) \no \nAmerican(Robert) ................................. (8) \n \nBackward-Chaining proof: \n \nIn Backward chaining, we will start with our goal predicate, which is Criminal(Robert), and then \ninfer further rules. \n \nStep-1: \n \nAt the first step, we will take the goal fact. And from the goal fact, we will infer other facts, \nand at last, we will prove those facts true. So our goal fact is \"Robert is Criminal,\" so \nfollowing is the predicate of it. \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 73 \n \n \nStep-2: \n \nAt the second step, we will infer other facts form goal fact which satisfies the rules. So as \nwe can see in Rule-1, the goal predicate Criminal (Robert) is present with substitution \n{Robert/P}. So we will add all the conjunctive facts below the first level and will replace p \nwith Robert. \nHere we can see American (Robert) is a fact, so it is proved here. \n \n \nStep-3:t At step-3, we will extract further fact Missile(q) which infer from Weapon(q), as it \nsatisfies Rule-(5). Weapon (q) is also true with the substitution of a constant T1 at q. \n \n \nStep-4: \n \nAt step-4, we can infer facts Missile(T1) and Owns(A, T1) form Sells(Robert, T1, r) which satisfies \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 74 \n \n \nthe Rule- 4, with the substitution of A in place of r. So these two statements are proved here. \n \n \n \nStep-5: \n \nAt step-5, we can infer the fact Enemy(A, America) from Hostile(A) which satisfies Rule- 6. \nAnd hence all the statements are proved true using backward chaining. \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 75 \n \nDifference between backward chaining and forward chaining \n \nForward Chaining \nBackward Chaining \nNo. \n1 \n. \nForward \nchaining \nstarts from known \nfacts and applies \ninference rule to \nextract more data \nunit it reaches to the \ngoal. \nBackward chaining \nstarts from the goal \nand \nworks \nbackward \nthrough \ninference rules to \nfind the required \nfacts that support \nthe goal. \n2 \n. \nIt is a bottom-up \napproach \nIt is a top-down \napproach \n3 \n. \nForward \nchaining \nis known as data-\ndriven \ninference \ntechnique as we \nreach to the goal \nusing the available \ndata. \nBackward chaining \nis known as goal-\ndriven technique as \nwe start from the \ngoal and divide into \nsub-goal to extract \nthe facts. \n4 \n. \nForward \nchaining \nreasoning applies a \nbreadth-first\n \nsearch strategy. \nBackward chaining \nreasoning applies a \ndepth- first search \nstrategy. \n5 \n. \nForward chaining \ntests for all the \navailable rules \nBackward chaining \nonly tests for few \nrequired rules. \n6 \n. \nForward chaining is \nsuitable \nfor \nthe \nplanning, \nmonitoring, control, \nand \ninterpretation \napplication. \nBackward chaining \nis \nsuitable \nfor \ndiagnostic, \nprescription,\n \nand debugging \napplication. \n7 \n. \nForward \nchaining \ncan generate \nan \ninfinite number of \npossible \nBackward \nchaining \ngenerates \na \nfinite \nnumberof \npossible \nconclusions. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 76 \n \nconclusions. \n8 \n. \nIt operates in\n the \nforward \ndirection. \nIt operates in\n the \nbackward \ndirection. \n9 \n. \nForward \nchaining \nis aimed for any \nconclusion. \nBackward chaining \nis only aimed for \nthe required data. \nBasic probability notation \n• \nPrior probability :We will use the notation P(A) for the unconditional or prior \nprobability that the proposition A is true. \n• \nFor example, if Cavity denotes the proposition that a particular patient has a cavity, \nP(Cavity) = means that in the absence of any other information, the agent will assign a \nprobability of \n0.1( a 10%chance) \n• \nIt is important to remember that P(A) can only be used when there is no other information. \nAs soon as some new information B is known, we have to reason with the conditional \nprobability of A given B instead of P(A) to the event of the patient's having a cavity. \n• \nPropositions can also include equalities involving so-called random variables. \n• \nFor example, if we are concerned about the random variable \nWeather, we might have P( Weather = Sunny) = 0.7 \nP(Weather = Rain) \n= 0.2 P(Weather= \nCloudy) \n= \n0.08 \nP(Weather \n= \nSnow) = 0.02 \nEach random variable X has a domain of possible values (x1,...,xn) that it can take on. \n• \nWe can view proposition symbols as random variables as well, if we assume that they \nhave a domain [true,false). \n• \nThus, the expression P(Cavity) can be viewed as shorthand for P(Cavity = true). \n• \nSimilarly, P(->Cavity) is shorthand for P(Cavity =false). \n• \nSometimes, we will want to talk about the probabilities of all the possible values of a \nrandom variable. In this case, we will use an expression such as P(Weather ) \n• \nfor \nexample, \nwe \nwould \nwrite \nP(Weather) \n= \n(0.7,0.2,0.08,0.02) This statement defines a probability \ndistribution \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 77 \n \n• \nWe can also use logical connectives to make more complex sentences and assign \nprobabilities to them. \nFor example, P(Cavity A ¬Insured) \nConditional probability: \n• \nOnce the agent has obtained some evidence concerning the previously unknown \npropositions making up the domain, prior probabilities are no longer applicable. \nInstead, we use conditional or posterior probabilities, with the notation P(A|B) \n \n \nThis is read as \"the probability of A given that all we know is B.\" \n• \nP(B|A) means \"Event B given Event A\" \n• \nIn other words, event A has already happened, now what is the chance of event B? \n• \nP(B|A) is also called the \"Conditional Probability\" of B given A. \nEx:Drawing 2 Kings from a Deck \n• \nEvent A is drawing a King first, and Event B is drawing a King second. \n• \nFor the first card the chance of drawing a King is 4 out of 52 (there are 4 Kings in a deck \nof 52 cards): \n• \nP(A) = 4/52 \n• \nBut after removing a King from the deck the probability of the 2nd card drawn is less \nlikely to be a King (only 3 of the 51 cards left are Kings): \n• \nP(B|A) = 3/51 \nAnd so: P(A and B) = P(A) x P(B|A) = (4/52) x (3/51) = 12/2652 = 1/221 \n• \nSo the chance of getting 2 Kings is 1 in 221, or about 0.5 \n \nBAYES Theorem: \n \n• \nBayes' Theorem is a way of finding a probability when we know certain other probabilities. \n \nThe formula is \n \n \n• \nWhich tells us: how often A happens given that B happens, written P(A|B), \n• \nWhen we know: How often B happens given that A happens, written P(B|A) \n• \nand how likely A is on its own, written P(A) \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n            DEPARTMENT OF CSE                                                                                                                                                  Page 78 \n \n• \nand how likely B is on its own, written \nP(B) Example: \n• \nDangerous fires are rare (1%) \n• \nBut smoke is fairly common (10%) due to barbecues,and 90% of dangerous fires make \nsmoke We can then discover the probability of dangerous Fire when there is Smoke: \n \nP(Fire|Smoke) =P(Fire) P(Smoke|Fire)/P(Smoke) \n \n=1% x 90/10%=9% \nSo it is still worth checking out any smoke to be \nsure. Example 2: \nYou are planning a picnic today, but the morning \nis cloudy Oh no! 50% of all rainy days start off \ncloudy! \nBut cloudy mornings are common (about 40% of days start cloudy) \n \nAnd this is usually a dry month (only 3 of 30 days tend to be \nrainy, or 10%) What is the chance of rain during the day? \nWe will use Rain to mean rain during the day, and Cloud to mean cloudy \nmorning. The chance of Rain given Cloud is written P(Rain|Cloud) \nSo let's put that in the formula: \n \nP(Rain|Cloud) \n= \nP(Rain) \nP(Cloud|Rain)/P(Cloud) \nP(Rain) \nis \nProbability of Rain = 10% \nP(Cloud|Rain) is Probability of Cloud, given that Rain \nhappens = 50% P(Cloud) is Probability of Cloud = 40% \nP(Rain|Cloud) = 0.1 x 0.5/0.4 = .125 \n \nOr a 12.5% chance of rain. Not too bad, let's have a picnic! \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 79 \n \nUNIT-III \n \nArtificial intelligence is a system that is concerned with the study of understanding, \ndesigning and implementing the ways, associated with knowledge representation to \ncomputers. \nIn any intelligent system, representing the knowledge is supposed to be an important technique \nto encode the knowledge. \nThe main objective of AI system is to design the programs that provide information to the \ncomputer, which can be helpful to interact with humans and solve problems in various fields \nwhich require human intelligence. \nWhat is Knowledge? \nKnowledge is an useful term to judge the understanding of an individual on a given subject. \nIn intelligent systems, domain is the main focused subject area. So, the system specifically \nfocuses on acquiring the domain knowledge. \nIssues in knowledge representation \nThe main objective of knowledge representation is to draw the conclusions from the \nknowledge, but there are many issues associated with the use of knowledge representation \ntechniques. \n \nRefer to the above diagram to refer to the following issues. \n \n1. Important attributes \nAdvanced Knowledge Representation and Reasoning: Knowledge Representation Issues, \nNonmonotonic Reasoning, Other Knowledge Representation Schemes Reasoning Under \nUncertainty: Basic probability, Acting Under Uncertainty, Bayes’ Rule, Representing Knowledge \nin an Uncertain Domain, Bayesian Networks \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 80 \n \nThere are two attributes shown in the diagram, instance and isa. Since these attributes support property of \ninheritance, they are of prime importance. \n \n2. Relationships among attributes \nBasically, the attributes used to describe objects are nothing but the entities. However, the \nattributes of an object do not depend on the encoded specific knowledge. \n3. Choosing the granularity of representation \nWhile deciding the granularity of representation, it is necessary to know the following: \ni. What are the primitives and at what level should the knowledge be represented? \n \nii. What should be the number (small or large) of low-level primitives or high-level facts? \nHigh-level facts may be insufficient to draw the conclusion while Low-level primitives may \nrequire a lot of storage. \nFor example: Suppose that we are interested in following facts: \nJohn spotted Alex. \nNow, this could be represented as \"Spotted (agent(John), object (Alex))\" \nSuch a representation can make it easy to answer questions such as: Who spotted \nAlex? Suppose we want to know : \"Did John see Sue?\" \nGiven only one fact, user cannot discover that answer. \nHence, the user can add other facts, such as \"Spotted (x, y) → saw (x, y)\" \n4. Representing sets of objects. \nThere are some properties of objects which satisfy the condition of a set together but not as \nindividual; Example: Consider the assertion made in the sentences: \n\"There are more sheep than people in Australia\", and \"English speakers can be found all over \nthe world.\" These facts can be described by including an assertion to the sets representing \npeople, sheep, and English. \n5. Finding the right structure as needed \nTo describe a particular situation, it is always important to find the access of right structure. \nThis can be done by selecting an initial structure and then revising the choice. \n \nWhile selecting and reversing the right structure, it is necessary to solve following problem \nstatements. They include the process on how to: \n \n Select an initial appropriate structure. \n Fill the necessary details from the current situations. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 81 \n \n Determine a better structure if the initially selected structure is not appropriate to fulfill other \nconditions. \n Find the solution if none of the available structures is appropriate. \n Create and remember a new structure for the given condition. \n There is no specific way to solve these problems, but some of the effective knowledge \nrepresentation techniques have the potential to solve them. \nNon Monotonic reasoning: \n● In Non-monotonic reasoning, some conclusions may be invalidated if we add some more \ninformation to our knowledge base. \n● Logic will be said as non-monotonic if some conclusions can be invalidated by adding more \nknowledge into our knowledge base. \n● Non-monotonic reasoning deals with incomplete and uncertain models. \n \n● \"Human perceptions for various things in daily life, \"is a general example of non-monotonic reasoning. \n \nExample: Let suppose the knowledge base contains the following knowledge: \n● Birds can fly \n \n● Penguins cannot fly \n \n● Pitty is a bird \n \nSo from the above sentences, we can conclude that Pitty can fly. \nHowever, if we add one another sentence into knowledge base \"Pitty is a penguin\", which \nconcludes \"Pitty cannot fly\", so it invalidates the above conclusion. \nACTING UNDER UNCERTAINTY \nAgents may need to handle uncertainity ,whether due to partial observability,non \ndetermininsm or combination of two. \nSummarizing Uncertainity: \nConsider the following Simple rule: \nToothache=> Cavity \nNot all the patients with toothaches have cavities ,some of them may have gum disease ,an abscess or \n \nsome other problems \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 82 \n \nToothache=>cavity V Gum Problem V Abscess….. \nUnfortunately in order to make the rule true we have to add an almost unlimited list of possible problems \n \nTrying to use first-order logic to cope with a domain like medical diagnosis thus fails for \nthree main reasons: \nLaziness: It is too much work to list the complete set of antecedents or consequents needed \nto ensure an exceptionless rule, and too hard to use the enormous rules that result. \nTheoretical ignorance: Medical science has no complete theory for the domain. \nPractical ignorance: Even if we know all the rules, we may be uncertain about a \nparticular patient because all the necessary tests have not or cannot be run. \nThe agent's knowledge can at best provide only a degree of belief in the relevant sentences. Our \nmain tool for dealing with degrees of belief will be probability theory, which assigns a \nnumerical degree of belief between 0 and 1 to sentences. \nProbability provides a way of summarizing the uncertainty that comes from our laziness and \nignorance. We may not know for sure what afflicts a particular patient, but we believe that \nthere is, say, an 80% chance—that is, a probability of 0.8—that the patient has a cavity if he \nor she has a toothache \n \nBASIC PROBABILITY NOTATION \n \nPrior probability We will use the notation P(A) for the unconditional or prior probability \nthat the proposition A is true. \nFor example, if Cavity denotes the proposition that a particular patient has a cavity, \nP(Cavity) = 0 \nmeans that in the absence of any other information, the agent will assign a probability of \n0.1 (a 10% chance) to the event of the patient's having a cavity. \nIt is important to remember that P(A) can only be used when there is no other information. As \nsoon as some new information B is known, we have to reason with the conditional probability \nof A given B instead of P(A). \nThe proposition that is the subject of a probability statement can be represented by a proposition \nsymbol, as in the P(A) example. Propositions can also include equalities involving so-called \nrandom variables. For example, if we are concerned about the random variable Weather, we \nmight have \n \n              P( Weather = Sunny) = 0.7 P(Weather = Rain) = 0.2 P(Weather= Cloudy) = 0.08 P(Weather = \nSnow) = 0.02 \nEach random variable X has a domain of possible values (x\\,...,xn) that it can take on \n \nWe can view proposition symbols as random variables as well, if we assume that they have a \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 83 \n \ndomain [true,false). Thus, the expression P(Cavity) can be viewed as shorthand for P(Cavity = true). \nSimilarly, \nP(->Cavity) is shorthand for P(Cavity =false). Usually, we will use the letters A, B, and so on for \nBoolean random variables, and the letters X, Y, and so on for multivalued variables. \n \nSometimes, we will want to talk about the probabilities of all the possible values of a \nrandom variable. In this case, we will use an expression such as P(Weather), which denotes \nvector of values for the probabilities of each individual state of the weather. \nGiven the preceding values, for example, we would write P(Weather) = \n(0.7,0.2,0.08,0.02) This statement defines a probability distribution for the random \nvariable Weather. \nWe will also use expressions' such as P(Weather, Cavity) to denote the probabilities of all \ncombinations of the values of a set of random variables. \nIn this case, P(Weather, Cavity) denotes a 4 x 2 table of probabilities. We will see that this \nnotation simplifies many equations. We can also use logical connectives to make more \ncomplex sentences and assign probabilities to them. For example, P(Cavity A -^Insured) - 0.06 \nsays there is an 6% chance that a patient has a cavity and has no insurance \nConditional probability: \n Once the agent has obtained some evidence concerning the previously unknown \npropositions making up the domain, prior probabilities are no longer applicable. Instead, we \nuse conditional or posterior probabilities, with the notation P(A|B). \n This is read as \"the probability of A given that all we know is B.\" \nFor example, indicates that if a patient is observed to have a toothache, and no other \ninformation is yet available, \nthen the probability of the patient having a cavity will be 0.8. \n It is important to remember that P(A|B) can only be used when all we know is B. As soon as we \nknow C, then we must compute \n P(A|B A C) instead of P(A|B). A prior probability P(A) can be thought of as a special \ncase of conditional probability P(A\\), where the probability is conditioned on no \nevidence. \n We can also use the P notation with conditional probabilities. P(X| Y) is a two-\ndimensional table giving the values of P(X=x,|Y = yj) for each possible I, j. Conditional \nprobabilities can be defined in terms of unconditional probabilities. The equation \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 84 \n \n \n \nAxioms of Probability: \n All probabilities are between 0 \nand 1. 0 < P(A) < 1 \n Necessarily true (i.e., valid) propositions have probability 1, and necessarily false \n(i.e., unsatisfiable) propositions have probability 0. P(True) = 1 P(False) = 0 \n The probability of a disjunction is given by P(A V 5) = P(A) + P(B) - P(A A B) \n \nThe joint probability distribution \nThe joint probability distribution (or \"joint\" for short), which completely specifies an agent's \nprobability assignments to all propositions in the domain (both simple and complex). \nA probabilistic model of a'domain consists of a set of random variables that can take on \nparticular values with certain probabilities. Let the variables be X\\ ... Xn. \nAn atomic event is an assignment of particular values to all the variables—in other words, a \ncomplete specification of the state of the domain \nThe joint probability distribution P(X],.. . ,Xn) assigns probabilities to all possible atomic \nevents. Recall that P(X,) is a one-dimensional vector of probabilities for the possible values of \nthe variable X,-. Then \n \nthe joint is an w-dimensional table with a value in every cell giving the probability of that \nspecific state occurring. Here is a joint probability distribution for the trivial medical domain \nconsisting of the two Boolean variables Toothache and Cavity: \n \n \nAdding across a row or column gives the unconditional probability of a variable, for example, P(Cavity) \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 85 \n \n= 0.06 + 0.04 = 0.10. \nP(Cavity V Toothache) = 0.04 + 0.01 + 0.06 = 0.11 \nBayes Rule: \n \n \nRepresenting knowledge in uncertain domain \nIn the context of using Bayes' rule, conditional independence relationships among variables can \nsimplify the computation of query results and greatly reduce the number of conditional \nprobabilities that need to be specified. We use a data structure called a belief BELIEF NETWORK \nnetwork' to represent the dependence between variables and to give a concise specification of the \njoint probability distribution. \nA Bayesian Network Is a directed graph in which each node is annotated with quantitative \nprobability information. \nThe full specification is as follows: \n1. \nEach node corresponds to a random variable , which can be discrete or continuous. \n2. \nf A set of directed links or arrows connects pairs of nodes. If there is an arrow from node X to \nnode Y ,X I s said to be parent of Y.The graph has no directed cycles and hence it is called \ndirected acyclic graph(DAG) \n3. \nEach node Xi has a conditional probability distribution P(Xi|Parents(Xi)) that quantifies the \neffect of the parents on the node. \nThe intuitive meaning of an arrow from node X to node Y is that X has a direct influence on Y \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 86 \n \n \n \nConsider the following situation. You have a new burglar alarm installed at home. It is fairly \nreliable at detecting a burglary, but also responds on occasion to minor earthquakes. (This \nexample is due to Judea Pearl, a resident of Los Angeles; hence the acute interest in \nearthquakes.) You also have two neighbors, John and Mary, who have promised to call you at \nwork when they hear the alarm. John always calls when he hears the alarm, but sometimes \nconfuses the telephone ringing with the alarm and calls then, too. \nMary, on the other hand, likes rather loud music and sometimes misses the alarm altogether. \nGiven the evidence of who has or has not called, we would like to estimate the probability of a \nburglary. \nThis simple domain is described by the belief network in Figure 15.2 \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 87 \n \nNotice that the network does not have nodes corresponding to Mary currently listening to loud \nmusic, or to the telephone ringing and confusing John. These factors are summarized in the \nuncertainty associated with the links from Alarm to JohnCalls and MaryCalls. \nThis shows both laziness and ignorance in operation: it would be a lot of work to determine \nany reason why those factors would be more or less likely in any particular case, and we have no \nreasonable way to obtain the relevant information anyway. \nThe probabilities actually summarize a potentially infinite set of possible circumstances in \nwhich the alarm might fail to go off (high humidity, power failure, dead battery, cut wires, dead \nmouse stuck inside bell,...) or John or Mary might fail to call and report it (out to lunch, on \nvacation, temporarily deaf, passing helicopter, ...). In this way, a small agent can cope with a \nvery large world, at least approximately. The degree of approximation can be improved if we \nintroduce additional relevant information. \n \nBayesian belief network \nBayesian belief network is key computer technology for dealing with probabilistic events and \nto solve a problem which has uncertainty. We can define a Bayesian network as: \n\"A Bayesian network is a probabilistic graphical model which represents a set of variables \nand their conditional dependencies using a directed acyclic graph.\" \nIt is also called a Bayes network, belief network, decision network, or Bayesian model. \nBayesian networks are probabilistic, because these networks are built from a probability \ndistribution, and also use probability theory for prediction and anomaly detection \nBayesian Network can be used for building models from data and experts opinions, and it \nconsists of two parts: \nDirected Acyclic Graph \nTable of conditional probabilities. \nThe generalized form of Bayesian network that represents and solve decision problems under \nuncertain knowledge is known as an Influence diagram. \nA Bayesian network graph is made up of nodes and Arcs (directed links), where: \n \n \no \nEach node corresponds to the random variables, and a variable can be continuous or discrete. \no \nArc or directed arrows represent the causal relationship or conditional probabilities \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 88 \n \nbetween random variables. These directed links or arrows connect the pair of nodes in \nthe graph. \nThese links represent that one node directly influence the other node, and if there is no directed \nlink that means that nodes are independent with each other \no \nIn the above diagram, A, B, C, and D are random variables represented by the nodes of the network \ngraph. \no \nIf we are considering node B, which is connected with node A by a directed arrow, then node \nA is called the parent of Node B. \no \nNode C is independent of node A. \nEach node in the Bayesian network has condition probability distribution P(Xi |Parent(Xi) \n), which determines the effect of the parent on that node. \nBayesian network is based on Joint probability distribution and conditional probability. So \nlet's first understand the joint probability distribution: \nJoint probability distribution: \nIf we have variables x1, x2, x3, ...., xn, then the probabilities of a different combination of x1, x2, x3.. \nxn, are known as Joint probability distribution. \nP[x1, x2, x3, .... , xn], it can be written as the following way in terms of the joint probability distribution. \n \n= P[x1| x2, x3,....., xn]P[x2, x3, .... , xn] \n \n= P[x1| x2, x3,....., xn]P[x2|x3,....., xn] ... P[xn-1|xn]P[xn]. \n \nIn general for each variable Xi, we can write the equation as: \n P(Xi|Xi-1, , X1) = P(Xi |Parents(Xi )) \nExplanation of Bayesian network: \nLet's understand the Bayesian network through an example by creating a directed acyclic graph: \n \nExample: Harry installed a new burglar alarm at his home to detect burglary. The alarm reliably \nresponds at detecting a burglary but also responds for minor earthquakes. Harry has two neighbors \nDavid and Sophia, who have taken a responsibility to inform Harry at work when they hear the \nalarm. David always calls Harry when he hears the alarm, but sometimes he got confused with the \nphone ringing and calls at that time too. On the other hand, Sophia likes to listen to high music, so \nsometimes she misses to hear the alarm. Here we would like to compute the probability of \nBurglary Alarm. \n \nProblem: \nCalculate the probability that alarm has sounded, but there is neither a burglary, nor an \nearthquake occurred, and David and Sophia both called the Harry. \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 89 \n \nSolution: \nThe Bayesian network for the above problem is given below. The network structure is showing that \nburglary and earthquake is the parent node of the alarm and directly affecting the probability of \nalarm's going off, but David and Sophia's calls depend on alarm probability. \nThe network is representing that our assumptions do not directly perceive the burglary and also do \nnot   notice the minor earthquake, and they also not confer before calling. \nThe conditional distributions for each node are given as conditional probabilities table or CPT. \nEach row in the CPT must be sum to 1 because all the entries in the table represent an exhaustive \nset of cases for the variable. \nIn CPT, a boolean variable with k boolean parents contains 2K probabilities. Hence, if there are two \nparents, then CPT will contain 4 probability values \nList of all events occurring in this network: \n \nBurglar\ny \n(B) \nEarthqu\nake(E) \nAlarm(\nA) \nDavid \nCalls(D\n) Sophia \ncalls(S) \nWe can write the events of problem statement in the form of probability: P[D, S, A, B, E], can \nrewrite the above probability statement using joint probability distribution: \nP[D, S, A, B, E]= P[D | S, A, B, E]. P[S, A, B, E] \n \n \n=P[D | S, A, B, E]. P[S | A, B, E]. P[A, B, E] \nP [D| A]. P [ S| A, B, E]. P[ A, B, E] \n \n= P[D | A]. P[ S | A]. P[A| B, E]. P[B, E] \n \n= P[D | A ]. P[S | A]. P[A| B, E]. P[B |E]. P[E] \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 90 \n \n \nLet's take the observed probability for the Burglary and earthquake \ncomponent: P(B= True) = 0.002, which is the probability of burglary. \nP(B= False)= 0.998, which is the probability of no burglary. \nP(E= True)= 0.001, which is the probability of a minor earthquake \nP(E= False)= 0.999, Which is the probability that an earthquake not occurred. \n \nConditional probability table for Alarm A: \n \nThe Conditional probability of Alarm A depends on Burglar and \nearthquake: B \nE \nP(A= True) P(A= False) \nT\nr\nu\ne \nT\nr\nu\ne\n0\n.\n9\n4\n0\n.\n0\n6\nT\nr\nu\ne \nF\na\nl\ns\ne\n0\n.\n9\n5\n0\n.\n0\n4\nF\na\nl\ns\ne \nT\nr\nu\ne\n0\n.\n3\n1\n0\n.\n6\n9\nF\nF\n0\n0\n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 91 \n \na\nl\ns\ne \na\nl\ns\ne\n.\n0\n0\n1\n.\n9\n9\n9\n \nConditional probability table for David Calls: \n \nThe Conditional probability of David that he will call depends on the probability of Alarm. \n \nA \nP(D= \nTrue) \nP(D= \nFalse) \nT\nr\nu\ne \n0.91 \n0.09 \nF\nal\ns\ne \n0.05 \n0.95 \n \nConditional probability table for Sophia Calls: \nThe Conditional probability of Sophia that she calls is depending on its Parent \nNode \"Alarm.\" A \nP(S= True) P(S= False) \nTrue 0.75 0.25 \nFalse 0.02 0.98 \n \nFrom the formula of joint distribution, we can write the problem statement in the form of \nprobability distribution: \n \nP(S, D, A, ¬B, ¬E) = P (S|A) *P (D|A)*P (A|¬B ^ ¬E) *P (¬B) *P (¬E). \n \n= 0.75* 0.91* 0.001* 0.998*0.999 \n \nfrom the formula of joint distribution, we can write the problem statement in the form of \nprobability distribution: \n \nP(S, D, A, ¬B, ¬E) = P (S|A) *P (D|A)*P (A|¬B ^ ¬E) *P (¬B) *P (¬E). \n \n= 0.75* 0.91* 0.001* 0.998* \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 92 \n \nUNIT-IV \n \nWhat is learning? \nMost often heard criticisms of AI is that machines cannot be called intelligent until theyare able \nto learn to do new things and adapt to new situations, rather than simply doing asthey are told to \ndo. \nSome critics of AI have been saying that computers cannot learn! \nDefinitions of Learning: changes in the system that are adaptive in the sense that they enable the \nsystem to do the same task or tasks drawn from the same population more efficiently and more \neffectively the next time. \n \nLearning covers a wide range of phenomenon: \n \nSkill refinement: Practice makes skills improve. More you play tennis, better you get \n \nKnowledge acquisition: Knowledge is generally acquired through experience \n \n \nVarious learning mechanisms: \n \nRote learning: \n \nRote Learning is basically memorisation. \n• Saving knowledge so it can be used again. \nLearning: What Is Learning? Rote Learning, Learning by Taking Advice, Learning in Problem \nSolving, \nLearning from Examples: Winston‘s Learning Program, Decision Trees. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 93 \n \n• Retrieval is the only problem. \n \n              No repeated computation, inference or query is necessary. \n• A simple example of rote learning is caching \n• Store computed values (or large piece of data) \n• Recall this information when required by computation. \n• Significant time savings can be achieved. \n• Many AI programs (as well as more general ones) have used caching very effectively. \nCheckers game: \n \n \nSamuel's Checkers program employed rote learning (it also used parameter \nadjustment which will be discussed shortly). \n \nA minimax search was used to explore the game tree. \n \nTime constraints do not permit complete searches. \n \nIt records board positions and scores at search ends. \n \nNow if the same board position arises later in the game the stored value can be \nrecalled and the end effect is that deeper searched have occurred. \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 94 \n \n \n \n \n \n \nRote learning is basically a simple process. However it does illustrate some \nissues that are relevant to more complex learning issues. \nOrganisation \n \n \n-- access of the stored value must be faster than it would be to recompute it. \nMethods such as hashing, indexing and sorting can be employed to enable this. \n \nE.g Samuel's program indexed board positions by noting the number of \npieces. Generalisation \n \n-- The number of potentially stored objects can be very large. We may \nneed to generalise some information to make the problem manageable. \nE.g Samuel's program stored game positions only for white to move. Also \nrotations along diagonals are combined \nLearning by taking advice: \nThis is a simple form of learning. Suppose a programmer writes a set of instructions \nto instruct the computer what to do, the programmer is a teacher and the computer \nis a student. \nOnce learned (i.e. programmed), the system will be in a position to do new things. \nThe advice may come from many sources: human experts, internet to name a few. \nThis type of learning requires more inference than rote learning. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 95 \n \nThe knowledge must be transformed into an operational form before stored in the knowledge base. \n \n FOO (First Operational Operationaliser), for example, is a learning system which is used \nto learn the game of Hearts. \n \nIt converts the advice which is in the form of principles, problems, and methods into \neffective executable (LISP) procedures (or knowledge). Now this knowledge is ready to \nuse. \n \nA human user first translates the advice from English into a representation \n \nThat foo can understand \nFor eg: ―Avoid taking \npoints‖ \nAvoid(take \npoints me)(trick) \nAchieve (not( during (trick)(take point-me))))) \n \n \nLearning in Problem Solving- learning by Parameter Adjustment \nMany programs rely on an evaluation procedure to summarise the state of search etc. \nGame playing programs provide many examples of this. \nHowever, many programs have a static evaluation function to get a score that achieves the desirable \nboard position. \nIn learning a slight modification of the formulation of the evaluation of the problem is \nrequired. Here the problem has an evaluation function that is represented as a polynomial \nof the form such as: \n \nThe ‗t‘ terms are the values that contribute to the evaluation. The ‗C‘ terms are the \ncoefficients (weights) that are attached to these values. \n \nBut many moves must have contributed to that final outcome, Even if the program \nwins it may have made some wrong moves along the way \n \nBecause of the limitations Samuel program did two things: \n \nWhen the program is in learning mode paly against the copy of itself, At the end \nof the game if the modified function won then the modified version is accepted \notherwise the old one is retained. \nPeriodically,one term in the scoring function was eliminated and replaced by another. \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 96 \n \n \n \nLearning in Problem Solving-Learning with macro operators: \n \nThe basic idea here is similar to Rote Learning:Avoid expensive recomputation \n \nMacro-operators can be used to group a whole series of actions into one. \n \nFor example: Making dinner can be described a lay the table, cook dinner, serve \ndinner. We could treat laying the table as on action even though it involves a sequence of \nactions. \n \nThe STRIPS problem-solving employed macro-operators in it's learning phase. \n \nConsider a blocks world example in which ON(C,B) and ON(A,TABLE) are true. \n \nSTRIPS can achieve ON(A,B) in four steps: \nUNSTACK(C,B), \nPUTDOWN(C), \nPICKUP(A), STACK(A,B) \nSTRIPS now builds a macro-operator MACROP with preconditions ON(C,B), \nON(A,TABLE), postconditions ON(A,B), ON(C,TABLE) and the four steps as its \nbody. \nMACROP can now be used in future operation. \nBut it is not very general. The above can be easily generalised with variables used in place of the \nblocks. However generalisation is not always that easy \n \nNon Serializable subgoals: \n \nNon serializability means that working on one subgoal will necessarily interfere \nwith previous solution to another subgoal \n \nMacro operators can be used for games like 8-Puzzle(foe ex we have correctly placed \n4 tiles and our job is to put fifth without disturbing the earlier tiles. \n \nA macro will not disturb 4 files externally (but within the macro tiles are disturbed). \n \nLearning in Problem Solving-Learning from chunking: \n \nChunking is similar to learnig with macro-operators. Generally, it is used by \nproblem solver systems that make use of production systems. \n \nA production system consists of a set of rules that are in if-then form. That is given a \nparticular situation, what are the actions to be performed. For example, if it is raining then take \numbrella \n \nTo solve a problem, a system will compare the present situation with the left hand side \nof the rules. If there is a match then the system will perform the actions described in the right \nhand\n                               side of the corresponding rule. \n \nProblem solvers solve problems by applying the rules. Some of these rules may be more \nuseful than others and the results are stored as a chunk. Chunking can be used to learn general \nsearch control knowledge \n \nSeveral chunks may encode a single macro-operator and one chunk may participate in a \nnumber of macro sequences. Chunks learned in the beginning of problem solving, may be used \nin the later stage. The system keeps the chunk to use it in solving other problems. \n \nSoar is a general cognitive architecture for developing intelligent systems. Soar \nrequires knowledge to solve various problems. It acquires knowledge using chunking \nmechanism \n \nAn impasse arises when the system does not have sufficient knowledge. Consequently, \nSoar chooses a new problem space (set of states and the operators that manipulate the states) in \na bid to resolve the impasse. \n \nWhile resolving the impasse, the individual steps of the task plan are grouped into larger \nsteps known as chunks. \n \nThe chunks decrease the problem space search and so increase the efficiency of \nperforming the task. \n \nin Soar, the knowledge is stored in long-term memory. Soar uses the chunking \nmechanism to create productions that are stored in long-term memory. \n \nA chunk is nothing but a large production that does the work of an entire sequence of \nsmaller ones. \n \nThe productions have a set of conditions or patterns to be matched to working memory \nwhich consists of current goals, problem spaces, states and operators and a set of actions to \nperform when the production fires \n \nChunks are generalized before storing. When the same impasse occurs again, the chunks \nso collected can be used to resolve it. \n \nLearning from Examples-Induction: \n Classification is a process of assigning to a particular input, to tha name of the class to which \nit belongs to. Classification is important component in many problem solving tasks. \n But often classification is embedded inside another \noperation. For eg: \n \nIf:the current goal is to get from place A to place B and there is a wall seperating two places \n \n \nThen look for a Doorway in the wall and through it. \n \nTo use this rule successfully, the system‘s matching routine must be able to identify an \nobject as a wall. Without this the rule can never be invoked. \n \nThen to apply the rule,the system must be able to recognize the a doorway. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 98 \n \n \nBefore classification is done , the classes it will use must be defined . This can be done in \nvariety of ways: \n \nIsolate a set of features that are relevent to task domain.Define each class by some values \nof these features. \nEg: for weather predictions the parameters can be of rainfall,sunny,cloudy \n \nIsolate a set of features that are relevant to the task domain.Define a class as a structure \ncomposed of those features. \nFor example if the task is to identify animals,the body of each type of animal can be stored \nas structure and various features like color, length of a neck can be represented.The task of \nconstructing class definitions is called concept learning or Induction. \nLet us the learn the techniques to define classes structurally. \nWinston’s Learning Program: \n \nWinston describes an early structural concept learning program. \nIts goal is to construct representations of the definitions of concepts in the \nblocks domain. For eg : it learned the concepts, House tent and Arch \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 99 \n \n \nTo objects marry if they have faces that touch each and they have a common edge. \nThe marry relation is critical in the definition of arch . It is the difference between the first arch \nand near miss arch structure. In fig 17.2 \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 100 \n \n \nDecision Trees: \n \n \n \n \n \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 101 \n \nUNIT-V \n \n \nWhat is an Expert System? \nAn expert system is a computer program that is designed to solve complex problems and \nto provide decision-making ability like a human expert.. \nIt performs this by extracting knowledge from its knowledge base using the reasoning and inference \nrules according to the user queries. \nThe system helps in decision making for complex problems using both facts and heuristics like \na human expert. \nIt is called so because it contains the expert knowledge of a specific domain and can solve any \ncomplex problem of that particular domain. \nThese systems are designed for a specific domain, such as medicine, science, etc. \nThe performance of an expert system is based on the expert's knowledge stored in its \nknowledge base. The more knowledge stored in the KB, the more that system improves its \nperformance. \nOne of the common examples of an ES is a suggestion of spelling errors while typing in the \nGoogle search box. \nExamples of the Expert System: \nMYCIN: It was one of the earliest backward chaining expert systems that was designed to \nfind the bacteria causing infections like bacteraemia and meningitis. It was also used for the \nrecommendation of antibiotics and the diagnosis of blood clotting diseases. \nPXDES: It is an expert system that is used to determine the type and level of lung cancer. To \ndetermine the disease, it takes a picture from the upper body, which looks like the shadow. \nThis shadow identifies the type and degree of harm. \nCaDeT: The CaDet expert system is a diagnostic support system that can detect cancer at early stages \nRepresenting and using Domain knowledge: \nThe R1 program internally called XCON, for eXpert CONfigurer was a production-rule-based system to \n \nassist in the ordering of DEC's VAX computer systems by automatically selecting the computer \nsystem components based on the customer's requirements. \nIt eventually had about 2500 rules. \nRule of Xcon that configures DEC VAX system \nIf: the most current active context is distributing mass bus devices and \nExpert Systems: Representing and Using Domain Knowledge, Shell, Explanation, Knowledge \nAcquisition. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 102 \n \nThere is a single-port disk drive that has not been assigned to a massbus and \nThere are no unassigned dual port disk drives and the number of devices that each mass bus \nshould support is known and, \nThere is a mass bus that has been assigned at least one disk drive and that should support \nadditional disk drives, \nAnd the type of the cable needed to connect the disk drive to the previous device on the \nmass bus is known \nThen: assign the disk drive to the massbus \n As RI is doing a design task ( in contrast to the diagnosis task performed by MYCIN)it is \nnot necessary to consider all the possible alternatives one good one is enough. As a result \nprobabilistic information is not necessary in R1; \nPROSPECTOR is a program that provides advice on mineral exploration. It‘s rule \nlooks like this: IF: magnetite and pyrite is disseminated or veinlet form is present \nThen( 2,-4) there is a favourable mineralization and texture for the \npropylitic stage Here each rule contains two estimates. \nThe first indicates that the presence of evidence described in the condition part of the rule \nsuggests the validity of the rules conclusion \nThe second measures the extent to which the evidence is necessary to the validity of the \nconclusion. 2 indicates the presence of the evidence is encouraging.. \n-4 indicates that the absence of the evidence is slightly discouraging \n \nReasoning with knowledge \n■ Expert systems exploit many of the representation and reasoning mechanisms that we have discussed. \n \n■ Because these programs are usually written primarily as rule based systems, forward \nchaining and backward chaining are usually used . \n \n■ For ex: MYCIN used backward chaining to discover what organisms are present. And then \nuses forward chaining to reason from the organisms to a treatment regime.. \n■ RI on the other hand uses Forward chaining. \n \nExpert system Shells \nA new expert system can be developed by adding domain knowledge to the shell. The figure \ndepicts generic components of expert system. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 103 \n \n Knowledge acquisition system: It is the first and fundamental step. It helps to \ncollect the experts knowledge required to solve the Problems and build the knowledge \nbase. \n Knowledge Base: This component is the heart of expert systems. It stores all factual and \nheuristic knowledge about the application domain. It provides with the various representation \ntechniques for all the data. \n Inference mechanism: Inference engine is the brain of the expert system. This component is \nmainly responsible for generating inference from the given knowledge from the knowledge base \nand produce line of reasoning in turn the result of the user's query. \n Explanation subsystem: This part of shell is responsible for explaining or justifying \nthe final or intermediate result of user query. It is also responsible to justify need of \nadditional knowledge \n User interface: It is the means of communication with the user. It decides the utility of expert system. \n \n \n \n              Building expert systems by using shells has significant advantages. It is always advisable to use \nshell to develop  expert system as it avoids building the system from scratch. \n To build an expert system using system shell, one needs to enter all necessary knowledge \nabout a task domain into the shell. \nExplanation: \nAn expert system is said to be effective when people can interact with it easily. \nTo facilitate the interaction ,the expert system must have the following two properties: \n1. Explain its reasoning: In many of the domains in which experts system operate ,people \nwill not accept results unless they have been convinced of the accuracy of the of the reasoning \nprocess that produced those results. \nAn expert system is said to be effective when people can interact with it easily. \n2. Acquire new knowledge and modifications of old knowledge: since expert systems derive \ntheir power from the richness of the knowledge bases they is exploit it ,it is extremely \nimportant that those knowledge bases be complete and as accurate as possible \nOne way to get the knowledge into a program is through interaction with the human expert. Or to \nhave a program that learns the expert behaviour from raw data. \n TEIRESIAS was the first program to support explanation and knowledge acquisition. \n \nTEIRESIAS served as a front end for the MYCIN expert system. \nThe program asks for a piece information that it needs in order to continue its reasoning \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 104 \n \nThe doctor wants to know why the program wants the information and later asks the how \nthe program arrived at a conclusion that it claimed had reached \n Mycirn attempts to solve its goal of recommending a therapy for a particular patient by first \nfinding the cause of the patient‘s illness. \n \nIt uses its production rules to reason backward from goals to clinical observations. \n \nTo solve the top level diagnostic goal, it looks for rules whose right side suggests diseases. \n \nIt then uses left sides of those rules(preconditions) to set up subgoals . \n \nThese subgoals are again matched against rules and their preconditions are used to set up \nadditional goals. \n \nWhenever a precondition specifies a specific piece of clinical evidence ,mycin uses that evidence, \n \notherwise it asks the user to provide the information. \n \nThe actual goal that MYCIN set up are more general than the they need to specify the \npreconditions of a individual rule. \nFor ex: \n \nIf a precondition satisfies  that the identity of a organism X , MYCIN will set up the \ngoal ―infer identity‖ \n \nThe first Question that the user asks is WHY? Why do you need to know that? \n \nBecause the clinical tests are either expensive or dangerous.. \n \nIt is important for the doctor to be convinced that the information is really needed before \nordering the test. \nBecause MYCIN is reasoning backward the question can be easily answered by examining the goal tree. \n● The user can ask the question How did you know that? \n \n● The question can be answered by looking at the goal tree and chaining backward from the \nstated fact to the evidence that allowed a rule that determined the fact to fire. \nKnowledge Acquisition: \n● How are experts system built? \n \nKnowledge Engineer Interviews domain experts and to get the clear knowledge and the \nthey are translated into rules- This process is expensive and time consuming. \n● Look for Automatic ways of constructing expert knowledge bases, but no automatic \nknowledge acquisition systems exist yet. \n● But there are programs that interact with domain experts to extract knowledge efficiently. \n \n● These programs provides supports for the following activities: \n \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 105 \n \n1. Entering knowledge \n2. maintaining Knowledge base consistency \n3. Ensuring Knowledge base completeness. \n● The most useful knowledge acquisition programs are those that are restricted to a particular \nproblem solving paradigm eg: diagnosis or design. \n● If the paradigm is diagnosis then the program can structure its knowledge base around \nsymptoms, hypothesis and causes. \n \n              It can identify symptoms for which the expert system has not yet provided causes. \n \n● Since one system have many multiple causes the program can ask for knowledge about \nhow to decide when one hypothesis is better than another. \n● MOLE is a knowledge acquisition system for heuristic classification problems, such as \ndiagnosing diseases. \n● It used in conjunction with COVER AND DIFFERENTIATE problem solving method. \n \n● An Expert system produced by MOLE accepts input data ,comes up with a set of candidate \nexplanations or classifications that cover(explain) the data., the uses differentiating knowledge \nto determine which one is best. \n● MOLE interacts with the human expert to produce a knowledge base that a system called \nMOLE- p(performance ) uses to solve problems \nThe acquisition proceeds through several steps: \n1. Initial knowledge base construction. MOLE asks the expert to list common \nsymptoms or complaints that might require diagnosis. \nFor each symptom ,MOLE prompts for a list of possible explanations. \nWhenever an event has multiple explanations, MOLE tries to determine the conditions under which \nthe explanation is correct. \nThe expert provides COVERING knowledge ,that is the knowledge that a hypothesized event \ndoes occur, then the symptom will definitely appear. \n2. Refinement of knowledge Base: \nMOLE now tries to identify the weaknesses of knowledge base.One approach is to find holes \nand prompt the expert to fill them. \nMoLE lets the expert watch MOLE-P solving sample problems. \nWhen ever MOLE-p makes an incorrect diagnosis ,the expert adds new knowledge. \nFor Ex: suppose we have a patient with Symptoms A and B. Futher suppose that symptom A \ncould be caused by the events X and Y, and that symptom B can be caused by Y and Z. \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 106 \n \nMOLE-p may conclude Y, since it explains both A and B. \nIf the expert indicates that this decision was incorrect,then MOLE will ask what evidence \nshould be used tp prefer X and/or Z over Y \n         Suppose if our task is to design an artifact for eg: an elevator system, then we must assign values to \nlarge number    of parameters such as width of the platform, the type of door,the cable weight, cable \nstrength. \n● These parameters must be consistent with each other and they must result in the design \nthat satisfies external constraints imposed by cost factors, the type of building involved and \nthe expected payloads. \n● One problem solving method useful for design tasks is called propose and Revise. \n \n● Here the system first proposes an extension to the current design. Then it checks whether \nthe extension violates any global or local constraints. \n● Constraints violations are fixed and the process repeats. \n \n● It turns out that domain experts are good at listing overall design constraints and \nproviding local constraints on the individual parameters ,but not so good at explaining how \nto arrive at global solutions. \n● The SALT program provides mechanisms for elucidating this knowledge from the expert. \n \n● SALT builds a dependency network as it converses with the expert. \n \n● Each node stands for a value of a parameter that must be acquired or generated. \n \n● There are three kinds of links: \n \n● Contributes–to ,constrains, suggests-revision-of \n \n● Contributes- to link are are procedures that allows SALT to generate a value for one \nparameter based on the value of another.. \n● Constrains rules out certain parameter values. \n \n● Suggests -revision- of linkpoints to ways in which a constrain violation can be fixed. \n \nSALT uses the following heuristics to guide the acquisition process: \n1. Every non-input node in the network needs atleast one link coming into it.If links are \nmissing the expert is asked to fill it. \n2. No contribute-to loops are allowed in the network.If a loop exists ,SALT tries to transform \n \nARTIFICIAL INTELLIGENCE                                                                                                                                                       A.Y 2023-2024          \n \n             ARTIFICIAL INTELLIGENCE                                                                                                                                             Page 107 \n \none of the contributes to links into constrains links. \n3. Constrains links should have Suggests-revision-of links associated with them. \n● These includes constrain links that are created when dependency loops are broken. \n \n● SALT compiles its dependency network into set of production rules.. \n \n● Consider a bank‘s problem in deciding whether to approve a loan a loan. \n \n● One approach to automate this task is to interview loan officers in an attempt to \nextract domain knowledge. \n● Another approach is to inspect the records of loans the bank has made in the past and try to \ngenerate rules automatically that will maximize the number of good loans and minimize the \nnumber of bad ones in the future. \n● META DENDRAL was the first program to use learning techniques to construct \nrules for the expert system automatically \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
  "1": "What are parallel and distributed systems?\nThey are types of computing architectures that enable multiple processors or computers to work together to perform tasks more efficiently than a single processor or computer could. \nThey both aim to solve large-scale computational problems, but they do so in different ways and are used for different types of applications.\nParallel Systems\nThese are computing systems where multiple processors execute or process an application or computation simultaneously. \nThey typically share a common memory space and work on the same task by dividing it into smaller sub-tasks that are processed concurrently.\nProcessors are tightly coupled, meaning they are typically located within the same physical machine and have high-speed interconnections.\nTasks running in parallel often need to synchronize their operations to ensure consistency and correctness, which can add complexity.\nExamples:\nGPUs (Graphics Processing Units): Used extensively in tasks like machine learning, where many simple operations are performed in parallel.\nApplications:\nWeather forecasting\nFinancial modelling\nDistributed Systems\nThey consist of multiple independent computers (or nodes) that work together to achieve a common goal. \nEach node has its own memory and processors, and they communicate with each other over a network, which can be slower and less reliable than the interconnections in a parallel system.\nNodes in a distributed system are loosely coupled and can be geographically dispersed. \nDistributed systems can easily scale out by adding more nodes to the network.\nExamples:\nCloud Computing: Services delivered over the internet that rely on distributed systems to provide scalable and flexible resources.\nPeer-to-Peer Networks: Systems like BitTorrent where each participant (peer) shares resources directly with others.\nApplications:\nSearch engines\nSocial media platforms\nBanking systems\nE-commerce platforms\nWhat is the Need of such systems\nPerformance Enhancement: Parallel and distributed systems offer a way to achieve higher computational throughput and reduced processing times by harnessing the power of multiple processors or machines working together.\nScalability: Applications today often deal with vast amounts of data and complex computations. Parallel and distributed systems enable scaling out (distributing tasks across multiple nodes) to handle increasing workloads efficiently.\nFault Tolerance and Reliability: Distributed systems are inherently more resilient to failures. By spreading tasks and data across multiple nodes, these systems can continue operating even if individual components fail, ensuring continuity and reliability.\nResource Utilization: Efficient utilization of computing resources is crucial, especially in real time environments. Parallel and distributed systems allow for better resource management, optimizing the use of CPUs, memory, and storage across a network of machines.\nReal-time Processing: Certain applications require real-time data processing and decision-making, such as in financial trading, online gaming, or autonomous vehicles. Distributed systems enable parallel processing of incoming data streams to meet these stringent latency requirements.\nCost Efficiency: By distributing workloads across multiple inexpensive commodity hardware units, parallel and distributed systems offer a cost-effective alternative to investing in expensive high-end servers for achieving similar computational capabilities.\nFlexibility and Adaptability: Distributed systems are flexible and adaptable to changes in workload or computing needs. They can dynamically allocate resources based on demand, ensuring optimal performance and efficiency under varying conditions.\nChallenges and Solutions in Parallel Programming\nClock Speed Saturation: Clock speeds have plateaued, but packing density (number of transistors) continues to increase.\nEffective Utilization: More cores don't automatically mean faster computations; efficient parallel programming is required.\nData Access Bottlenecks: Memory speeds haven't kept pace with processor speeds, leading to a bottleneck. Parallel architectures help mitigate this by distributing the load across multiple processors and memory chips.\nCommunication in Parallel Programming\nShared Memory vs. Message Passing:\nShared Memory: Common address space where data can be written and read by multiple processors.\nMessage Passing: Data is sent and received through explicit messages between processors.\nProgramming Models: OpenMP for shared memory and MPI for message passing are common tools used in parallel programming.\nConcurrent Programming Concerns\nRace Conditions: Occurs when multiple processes access and modify shared resources concurrently, leading to inconsistent results.\nSynchronization: Necessary to manage access to shared resources and ensure correct program behavior.\nArchitectural Considerations\nMemory Access and Data Communication: Performance can be impacted by the speed of memory access and data transfer between processors.\nTarget Models: Algorithms need to be designed with specific hardware architectures in mind for optimal performance.\nAmdahl's Law\nAmdahl's Law states that the speedup of a program is limited by the fraction of the code that cannot be parallelized. Even a small sequential fraction can significantly limit the speedup achievable with a large number of processors.\nLevels of parallelism\nBit-level \nA form of parallel computing based on increasing processor size to reduce the number of instructions required to perform tasks on large-sized data.\nExample:\n8-bit Processor:\nTask: Compute the sum of two 16-bit integers.\nSteps:\nSum the 8 lower-order bits.\nAdd the 8 higher-order bits.\nInstructions Required: 2 instructions.\n16-bit Processor:\nTask: Compute the sum of two 16-bit integers.\nSteps:\nSum all 16 bits at once.\nInstructions Required: 1 instruction.\nInstruction Level\nThe ability of a processor to execute multiple instructions simultaneously during a single clock cycle.\nInstructions can be re-ordered and grouped for concurrent execution. The re-ordering and grouping do not affect the final result of the program.\nTo maximize the utilization of CPU resources and improve performance by executing more than one instruction per clock cycle.\nExample: a = b + c; d = e - f; g = h * i;\nLoop Level\nInvolves iterative loop operations.\nSome loop-independent operations can be vectorized for pipelined execution.\nLoop level parallelism is highly optimized for parallel or vector computers.\nExample: \nfor (int i = 0; i < n; i++) {\n    C[i] = A[i] + B[i];\n}\n\nfor (int i = 0; i < n; i += 4) {\nC[i] = A[i] + B[i];\nC[i+1] = A[i+1] + B[i+1];\nC[i+2] = A[i+2] + B[i+2];\nC[i+3] = A[i+3] + B[i+3];\n}\nProcedural/Task Level\nA parallel computing model where a task is decomposed into subtasks, which are then executed concurrently by multiple processors.\nMechanism:\nDecomposition: Break down a large task into smaller, independent subtasks.\nAllocation: Assign each subtask to a different processor.\nProcessors execute the subtasks concurrently.\nIncreases the overall efficiency and speed of task execution by leveraging multiple processors.\nExample: Suppose we have a large image that needs to be processed (e.g., applying a filter) and the image can be divided into smaller sections for parallel processing.\nSubprogram Level\nRelates to job steps and associated subprograms.\nCan overlap across various jobs.\nUtilized in both uniprocessor and multiprocessor environments.\nExample: Consider a scenario where a data processing pipeline involves multiple steps, each with associated subprograms. Suppose we have a pipeline that processes customer orders with steps for validation, processing, and logging.\nJob/Program Level\nParallel execution of independent tasks on a parallel computer.\nHandled by program loader and operating system.\nExplored through time-sharing and space-sharing multiprocessors.\nExample: Consider a scenario where a server needs to handle multiple independent services, such as a web server, database server, and file server. Each service runs as an independent program and can be executed in parallel on a multiprocessor system.\nData Level \nA form of parallel computing where instructions from a single instruction stream operate concurrently on multiple data elements.\nMechanism:\nSingle Instruction Stream: One set of instructions is applied to multiple data points simultaneously.\nConcurrency: Achieves parallelism by processing multiple data elements in parallel.\nLimitations:\nNon-regular Data Manipulation Patterns: Irregular data access and manipulation patterns can hinder the effectiveness of DLP.\nMemory Bandwidth: The available memory bandwidth can limit the extent to which DLP can be exploited.\nExample: Applying a filter to an image involves performing the same operation on each pixel independently.\nParallelism in Uniprocessor Systems\nA computer system with a single central processing unit (CPU) that executes one instruction at a time are termed as uniprocessor systems.\nExamples: Personal computers, mobile devices, and small embedded systems.\nLimitations: Limited computing power due to sequential instruction execution.\nApplications: Web browsing, word processing, and basic gaming\nThe ability to execute multiple instructions simultaneously by dividing tasks among different processors is known as achieving parallelism.\nHow it is achieved in Uniprocessor: Through techniques like using multi cores in single processor, dividing process into smaller tasks, pipelining, and multitasking.\nParallelism Techniques in Uniprocessor:\nPipelining:\nDivides the instruction execution process into stages, allowing different stages to work on different instructions simultaneously.\nBenefits: Increases processor throughput and performance by overlapping instruction fetch and execution.\nMultitasking:\nDivides the processor’s time into short time slots and switches tasks rapidly. Each task is given a specific time slot in which it needs to be executed. \nBenefits: Provides an illusion of parallel execution, enhancing overall system responsiveness.\nPipelining\nA technique for decomposing a sequential process into sub-operations, where each sub-operation is executed in a dedicated segment that operates concurrently with other segments.\nKey Characteristics:\nConcurrent Execution: Several computations can be in progress simultaneously in different segments.\nPartial Processing: Each segment performs part of the processing as dictated by the task.\nData Transfer: Results from each segment are transferred to the next segment in the pipeline.\nFinal Result: Obtained after data has passed through all segments.\nEach segment consists of an input register followed by a combinational circuit.\nThe output of the combinational circuit in one segment is applied to the input register of the next segment.\nExample operation: Ai × Bi + Ci for i=1, 2, 3, … ,7\nSegment 1: Load A1​ and B1 into registers R1 and R2.\nSegment 2: Multiply A1 × B1 and store the result in R3, then load C1 into R4.\nSegment 3: Add R3 + R4 and store the result in R5.\n\nPipeline Speedup:\nNon-Pipelined Machine:\nTime required to complete n tasks: τnp = n × k × tp\nPipelined Machine (k stages):\nTime required to complete n tasks: τp = (k+n−1) × tp \nSpeedup S:\nS = (n × k × tp)/ (k + n - 1) × tp​​\nWhen n is much larger than k:\nS = n × k / n\nS = k\n\nAdvantages of Parallelism in Uniprocessor:\nImproved Performance: Allows a uniprocessor to handle multiple tasks simultaneously, reducing task completion times and increasing overall throughput.\nCost Effectiveness: Provides performance enhancements at a lower cost compared to multiprocessor systems.\nLower Power Consumption: Consumes less power, making it suitable for mobile and battery-powered devices.\nDisadvantages of Uniprocessor:\nLimited Scalability: Performance degrades as the number of concurrently executed tasks increases, limiting applications requiring high parallelism.\nLimited Processing Power: Not suitable for tasks needing extensive computational power like scientific simulations or large-scale data processing.\nComplex Design: Implementing parallelism requires careful system design and optimization, increasing development and maintenance costs.\nApplications of Parallelism in Uniprocessor:\nMultimedia Applications: Enhances performance in video playback, image processing, and 3D graphics rendering.\nWeb Servers: Enables handling multiple client requests concurrently, improving reliability.\nArtificial Intelligence and Machine Learning: Speeds up data processing tasks in AI and ML applications.\nScientific Simulations: Supports simulations in weather forecasting, fluid dynamics, and molecular modeling.\nDatabase Management Systems: Improves efficiency in handling large datasets.\nComponents of Parallel Architecture\nProcessor and Memory:\nPrivate memory for each processor.\nShared memory accessible by multiple processors.\nCommunication via memory or direct interconnects (wires, switches, networks).\nControl:\nDetermines how multiple processors are coordinated.\nCentralized vs. distributed control.\nFlynn’s Classification\nParallel computing systems are classified into four categories based on the number of instruction and data streams they can process simultaneously:\nSingle-Instruction, Single-Data (SISD) Systems:\nDescription: Uniprocessor systems that execute a single instruction on a single data stream sequentially.\nCharacteristics: Limited by the rate of internal data transfer.\nStorage: Instructions and data to be processed are stored in primary memory.\nProcessing: Instructions are processed in a sequential manner and such computers are called sequential computers.\nExamples: IBM PC, workstations.\nUse Case: Conventional sequential computing.\n\nMultiple-Instruction, Single-Data (MISD) Systems:\nDescription: Multiprocessor systems executing different instructions on different Processing Elements (PE)s on the same data stream.\nCharacteristics: Rare and not commercially available.\nExample Task: Z = sin(x) + cos(x) + tan(x).\nUse Case: Few applications, primarily theoretical.\n\nSingle-Instruction, Multiple-Data (SIMD) Systems:\nDescription: Multiprocessor systems that execute the same instruction on all CPUs on multiple data streams simultaneously.\nCharacteristics: Organize vector data into N sets for N Processing Elements (Pes), allowing each PE to process one set simultaneously.\nThey have multiple processing or executing units, but one control unit\nExamples: Cray’s vector processing machine.\nUse Case: Scientific computing.\nAdvantages:\nAn application that may take advantage of SIMD is one where same value is being added to or subtracted from a large number of data points, which is a common operation in many multimedia applications.\nProcessing multiple data at the same time with single instruction can drastically improve performance\nDisadvantages\nLarger register size\nMore power consumption\nRequire larger chip area\n\n\nMultiple-Instruction, Multiple-Data (MIMD) Systems:\nDescription: Multiprocessor systems that execute multiple instructions on multiple data streams.\nCharacteristics: Each processor element (PE) has separate instruction and data streams; PEs work asynchronously.\nThese processors have multiple processing cores (up to 61 which was recorded in 2015) \nExamples: Sun/IBM’s SMP (Symmetric Multi-Processing).\nAdvantages:\nLess contention \nHighly scalable \nOffers flexibility\nDisadvantages\nLoad balancing\nDeadlock situation prone\nWaste of bandwidth.\n\n\n\nMIMD Systems:\nCategories:\nShared-Memory MIMD:\nArchitecture: Tightly coupled; all PEs connected to a single global memory.\nCommunication: Through shared memory; modifications by one PE visible to all others.\nAdvantages: Easier to program.\nDisadvantages: Less fault-tolerant, harder to extend, prone to memory contention.\nExamples: Silicon Graphics machines, Sun/IBM’s SMP.\nDistributed-Memory MIMD:\nArchitecture: Loosely coupled; each PE has local memory.\nCommunication: Through interconnection network (IPC).\nNetwork Configurations: Tree, mesh, or other topologies as required.\nAdvantages: More fault-tolerant, easier to isolate failures, better scalability.\nDisadvantages: More complex to program.\nSuperiority: Practical outcomes and user requirements often favor distributed memory MIMD over other models.\n\nDistributed System Models\nPhysical Model\nArchitectural Model\nFundamental Model\n1. Physical Model\nA physical model represents the underlying hardware elements of a distributed system, including the hardware composition and interconnections. It is used for designing, managing, implementing, and determining the performance of a distributed system.\nComponents of the Physical Model:\nNodes\nEnd devices process data, execute tasks, and communicate with other nodes.\nExamples: computers, servers, workstations.\nAccess to services like storage, processing, and web browsing.\nLinks\nCommunication channels between nodes.\nCan be wired (copper wires, fiber optic cables) or wireless.\nTypes:\nPoint-to-Point Links: Connection between two nodes.\nBroadcast Links: Single node transmits data to multiple nodes simultaneously.\nMulti-Access Links: Multiple nodes share the same communication channel.\nMiddleware\nSoftware installed and executed on nodes to achieve decentralized control and decision-making.\nHandles communication, resource management, fault tolerance, synchronization, and security.\nNetwork Topology\nDefines the arrangement of nodes and links.\nCommon topologies: bus, star, mesh, ring, hybrid.\nCommunication Protocols\nRules and procedures for transmitting data.\nExamples: TCP, UDP, HTTPS, MQTT.\n\n\n\n\n2. Architectural Model\nThe architectural model describes the design and structure of the system, detailing how components interact and provide functionalities. It focuses on efficient cost usage and improved scalability. It aims to ensure the structure can meet current and future demands. The architectural model describes responsibilities distributed between system components and their placement. There are two main types: Client-Server Model and Peer-to-Peer Model.\nClient-Server Model\nCentralized approach where clients request services and servers provide them.\nWorks on request-response model implemented with send/receive primitives, RPC.\nUsed in web services, cloud computing, and database management systems.\nPeer-to-Peer Model\nDecentralized approach where nodes (peers) are equal in computing capabilities.\nNodes can request and provide services.\nHighly scalable and dynamic.\nInteraction patterns depend on the specific application.\nShared data objects are distributed across many computers, with each holding a small part of the database.\nDistributes processing and communication loads across computers and access links.\nOffers a flexible model addressing shared resource distribution and load balancing.\nExample: BitTorrent.\nLayered Model\nOrganizes the system into multiple layers, each providing specific services.\nLayers communicate with adjacent layers using well-defined protocols.\nExample: In a web application, the presentation layer handles the user interface\nMicroservices Model\nDecomposes complex applications into multiple independent services.\nEach service focuses on a specific function.\nEnhances maintainability, scalability, and understanding.\nExample: In an e-commerce application, separate microservices handle user authentication, product catalog, order processing, and payment processing. Each service operates independently and can be developed, deployed, and scaled separately.\nTwo-Tier Architecture:\nSplits application logic between client and server processes.\nOffers low latency but requires splitting application logic across a process boundary.\nThree-Tier Architecture:\nMaps logical elements to physical servers, enhancing maintainability.\nInvolves managing three servers and added network traffic and latency.\nThin Clients:\nTrend towards moving complexity from end-user devices to Internet services, supporting sophisticated networked services with minimal client device demands.\n\n3. Fundamental Model\nThe fundamental model provides a conceptual framework to understand key aspects of distributed systems. It includes the essential components required to understand the system's behavior.\nTypes:\nInteraction Model\nFramework for communication and coordination among processes.\nMessage Passing: Passing data, instructions, or service requests between nodes.\nPublish/Subscribe Systems: Publishing processes send messages on a topic, and subscribing processes receive them.\nSynchronous Systems: Have bounded execution times, known message delivery times, and bounded clock drift rates. Suitable for real-time applications.\nExample: A factory automation system where robots and machines need to operate in precise time coordination. Each machine must perform its tasks within a specific time frame to ensure the production line runs smoothly and efficiently.\nAsynchronous Systems: No bounds on execution times, message delivery, or clock drift rates. Suitable for many Internet-based systems.\nExample: Email communication. When you send an email, there is no guarantee when the recipient will receive or read it. The delivery and response times can vary greatly, but this is acceptable for the use case since immediate response is not required.\t\nRemote Procedure Call (RPC)\nAllows invoking a process or method on a remote server as if it were local.\nAbstracts message passing protocols for seamless communication between client and server.\nKey Concepts:\nStubs:\nClient Stub: Represents the server function in the client’s address space. It packages the procedure parameters into a message and sends it to the server.\nServer Stub: Represents the client’s call on the server side. It unpacks the message, performs the procedure, and sends the result back to the client.\nMarshalling/Unmarshalling:\nMarshalling: The process of gathering and encoding procedure parameters into a message format that can be sent over the network.\nUnmarshalling: The process of decoding the received message back into procedure parameters.\nCommunication:\nUtilizes protocols such as TCP or UDP for message exchange between the client and server.\nSynchronous/Asynchronous:\nSynchronous RPC: The client waits (blocks) for the server to finish processing and return the result.\nAsynchronous RPC: The client can continue processing and will be notified or can check back later for the result.\nSteps in RPC:\nClient Call:\nThe client program makes a local procedure call to the client stub.\nMarshalling:\nThe client stub marshals the parameters and sends the request message to the server.\nServer Reception:\nThe server stub receives the message, unmarshals the parameters, and makes the local procedure call on the server.\nServer Processing:\nThe server executes the requested procedure and generates a response.\nResponse Handling:\nThe server stub marshals the response and sends it back to the client.\nClient Reception:\nThe client stub receives the response, unmarshals it, and returns the result to the client program.\nExample:\nConsider a scenario where a desktop application needs to fetch user data from a remote database server.\nClient: The desktop application calls a procedure getUserData(userId) locally.\nClient Stub: The call is intercepted by the client stub, which marshals userId and sends it to the remote server.\nNetwork Transmission: The request message travels over the network to the server.\nServer Stub: The server stub receives the message, unmarshals userId, and calls the actual getUserData(userId) procedure on the server.\nServer Processing: The server retrieves the user data from the database.\nResponse: The server stub marshals the user data into a response message and sends it back to the client.\nClient Reception: The client stub receives the response, unmarshals the user data, and returns it to the application, which can now display or process the user data.\nAdvantages:\nTransparency: RPC abstracts the complexities of network communication, making remote interactions appear as local calls.\nModularity: Encourages modularity and reusability by separating client and server logic.\nLanguage Independence: RPC can work across different programming languages and platforms.\nChallenges:\nLatency: Network communication introduces latency.\nError Handling: Failures in network or server must be handled carefully.\nSecurity: Data transmitted over the network must be secured to prevent unauthorized access.\nComplexity: Implementing stubs, marshalling, and handling network communication adds complexity to system design.\nFailure Model\nFramework to identify and rectify faults in the system.\nTypes of Failures:\nCrash Failures: Process or node stops unexpectedly. Server halts while it starts working correctly.\nExample: Imagine a web server handling requests suddenly crashes due to a hardware malfunction. As a result, the server stops processing incoming requests until it is restarted, causing downtime for users trying to access the website.\nOmission Failures: Loss of messages. Server fails to respond to incoming requests or messages.\nExample: In a distributed messaging system, a message sent from one server to another is lost due to network issues. The receiving server never gets the message, leading to a lack of communication between components of the system.\nTiming Failures: Deviations in expected time quantum. Server's response time lies outside the specified interval.\nExample: A real-time trading application requires that trades be processed within 100 milliseconds. If a trade request takes 150 milliseconds to process, it might miss the optimal trading window, potentially resulting in financial loss.\nByzantine Failures: Malicious or unexpected messages.\nExample: In a blockchain network, a node intentionally sends false or conflicting transactions to disrupt consensus. This can lead to inconsistencies in the blockchain and undermine trust in the system's integrity.\nResponse Failure: Server's response is incorrect, with potential arbitrary behavior.\nExample: A database query is expected to return a list of user records, but instead, it returns an error or a completely unrelated set of data. This incorrect response could lead to application crashes or incorrect information being displayed to users.\nSecurity Model\nFramework for understanding security requirements, threats, and mechanisms. Addresses attacks from both external and internal agents, providing a basis for designing resilient systems.\nKey Aspects:\nAuthentication: Verifying user identities.\nEncryption: Transforming data to protect it from unauthorized access.\nData Integrity: Ensuring data remains unchanged during storage, transmission, or processing.\nHappened Before and Potential Causality Model\nThis model defines the order of events in a distributed system based on causality relationships.\nLamport Timestamps\nDeveloped by Leslie Lamport, this concept uses logical clocks to order events.\nEach process in the system maintains a counter (logical clock) that increments with each event.\nHappened-Before Relation (→)\nIf event A happened before event B in the same process, then A → B.\nIf both events occur on the same node, the local execution order determines the order (e.g., event A before B on the same node).\nIf event A is the sending of a message and event B is the receipt of that message, then A → B.\nThe happened-before relation is transitive: if A → B and B → C, then A → C.\n\nPotential Causality\nThis concept is inspired by physics where events are reasoned about in terms of causality and space-time. Events outside each other's \"light cone\" (in different space-time regions) cannot influence each other, similar to how distributed systems handle message dependencies.\nEvents can be in a strict order (one happening before the other) or concurrent (neither event knows about the other when it happens represented as A||B). This relationship is called a \"partial order.\"\nEvent A can potentially cause event B if A → B.\nEvents that are not causally related are considered concurrent.\nVector Clocks\nExtension of Lamport timestamps to capture causality more accurately.\nEach process maintains a vector of logical clocks.\nHelps in detecting causality violations and inconsistencies.\nExamples\nProcess P1 sends a message to Process P2.\nEvents in P1: A, B (send message), C.\nEvents in P2: D, E (receive message), F.\nCausality Analysis\nA → B → C (within P1)\nB → E (message send/receive)\nD → E → F (within P2)\nA → F (transitive relation through B and E)\nEvents A and D are concurrent.\nModels Based on States\nThese models describe the system in terms of states and state transitions.\nKey concepts include global state, local state, and state transitions.\nGlobal State\nA snapshot of the entire system at a particular instant.\nConsists of the local states of all processes and the state of the communication channels.\nLocal State\nThe state of an individual process, including variables, program counters, and local memory.\nCapturing Global State\nChandy-Lamport Algorithm\nUsed to capture a consistent global state in a distributed system.\nProcesses record their local state and send markers to other processes to record their states.\nAssumptions of the algorithm:\nThere are finite number of processes in the distributed system and they do not share memory and clocks.\nThere are finite number of communication channels and they are unidirectional and FIFO ordered.\nThere exists a communication path between any two processes in the system\nOn a channel, messages are received in the same order as they are sent.\nChandy-Lamport Algorithm Steps\nThe initiator process:\nProcess records its own local state.\nFor each outgoing channel from process, it sends a marker along C. \n(Note: Process Q will receive this marker on his incoming channel.)\nStarts recording the messages it receives on all its incoming channels.\nMarker receiving rule for a process Pi on channel Cki :\nIf process Pi has not yet recorded its own local state (seen (sent or received) the marker for first time) then:\nProcess records its own local state.\nRecord the state of incoming channel Cki as empty sequence.\nPi sends a marker on all its outgoing channels. \nPi starts recording on all incoming channels except Cki.\nIf process Pi has already recorded its state (seen (sent or received) the marker already) then:\nRecord the state of incoming channel Cki as the sequence of messages received after the state of Pi was recorded and before Pi received the marker along Cki.\nIf there are no messages over the channel then stop recording  on the channel Cki.\nNeed of taking snapshot or recording global state of the system:\nCheckpointing: It helps in creating checkpoint. If somehow application fails, this checkpoint can be reused\nGarbage collection: It can be used to remove objects that do not have any references.\nIt can be used in deadlock and termination detection.\nIt is also helpful in other debugging.\nConsistent Global State\nA global state that can be obtained through a series of consistent cuts (logical time slices) across the system.\nEnsures no messages are lost or duplicated in the recorded state.\nChallenges and Considerations\nHappened Before and Potential Causality\nScalability: Managing and synchronizing logical clocks in large systems.\nAccuracy: Ensuring vector clocks accurately represent causal relationships.\nState-based Models\nOverhead: Capturing and storing global states can be resource-intensive.\nCoordination: Ensuring consistent cuts across distributed processes.\nCombining Models\nSome systems use a combination of both models to leverage their strengths.\nExample: Distributed databases might use vector clocks for version control and state-based models for snapshots.\n",
  "2": "{\"company\": \"Offline AI\", \"description\": \"We build offline multilingual AI chatbots.\", \"founder\": \"Karthik\"}"
}